#!/usr/bin/env python3
"""
æ…§ (Hui) - PreCompact Hook è„šæœ¬
åœ¨ Claude Code è‡ªåŠ¨å‹ç¼©ä¸Šä¸‹æ–‡å‰è§¦å‘ï¼Œæå–å¹¶ä¿å­˜å…³é”®ä¿¡æ¯ã€‚

ä½¿ç”¨æ–¹æ³•:
1. å°†æ­¤è„šæœ¬æ”¾åˆ° ~/.wukong/hooks/hui-extract.py
2. åœ¨ .claude/settings.json ä¸­é…ç½®:
   {
     "hooks": {
       "PreCompact": [{
         "matcher": "auto",
         "hooks": [{
           "type": "command",
           "command": "python3 ~/.wukong/hooks/hui-extract.py"
         }]
       }]
     }
   }
"""

import json
import os
import sys
import re
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any


# ============================================================
# åˆ†èº«è¾“å‡ºå‹ç¼© (Avatar Output Compression)
# ============================================================

# ============================================================
# ç”¨æˆ·çº§åˆ«ä¸Šä¸‹æ–‡è·¯å¾„ (User-Level Context Paths)
# ============================================================
# æ‰€æœ‰ä¸Šä¸‹æ–‡å­˜å‚¨åœ¨ç”¨æˆ·ç›®å½•ï¼Œé¿å…é¡¹ç›®çº§å†²çªï¼Œæ”¯æŒè·¨é¡¹ç›®æ²‰æ·€

def get_user_context_dir() -> Path:
    """è·å–ç”¨æˆ·çº§åˆ«çš„ä¸Šä¸‹æ–‡æ ¹ç›®å½•"""
    return Path.home() / '.wukong' / 'context'


def get_project_name(cwd: str) -> str:
    """ä»å·¥ä½œç›®å½•æå–é¡¹ç›®åï¼ˆæœ€åä¸€çº§ç›®å½•åï¼‰"""
    return Path(cwd).name or 'unknown'


def get_active_session_dir(session_id: str) -> Path:
    """è·å–æ´»è·ƒä¼šè¯ç›®å½•ï¼ˆæŒ‰ session_id éš”ç¦»ï¼Œé¿å…å¤šä¼šè¯å†²çªï¼‰"""
    active_dir = get_user_context_dir() / 'active' / session_id
    active_dir.mkdir(parents=True, exist_ok=True)
    return active_dir


def get_sessions_archive_dir() -> Path:
    """è·å–å†å²ä¼šè¯å­˜æ¡£ç›®å½•"""
    sessions_dir = get_user_context_dir() / 'sessions'
    sessions_dir.mkdir(parents=True, exist_ok=True)
    return sessions_dir


def get_project_anchors_path(cwd: str) -> Path:
    """è·å–é¡¹ç›®çº§é”šç‚¹æ–‡ä»¶è·¯å¾„"""
    project_name = get_project_name(cwd)
    anchors_dir = get_user_context_dir() / 'anchors' / 'projects'
    anchors_dir.mkdir(parents=True, exist_ok=True)
    return anchors_dir / f'{project_name}.md'


def get_global_anchors_path() -> Path:
    """è·å–å…¨å±€é”šç‚¹æ–‡ä»¶è·¯å¾„"""
    anchors_dir = get_user_context_dir() / 'anchors'
    anchors_dir.mkdir(parents=True, exist_ok=True)
    return anchors_dir / 'global.md'


def get_session_index_path() -> Path:
    """è·å–ä¼šè¯ç´¢å¼•æ–‡ä»¶è·¯å¾„"""
    context_dir = get_user_context_dir()
    context_dir.mkdir(parents=True, exist_ok=True)
    return context_dir / 'index.json'


# ============================================================
# åˆ†èº«è¾“å‡ºå‹ç¼©é…ç½® (Avatar Output Compression Config)
# ============================================================

# åˆ†èº«ç±»å‹å¯¹åº”çš„å‹ç¼©é…ç½®
AVATAR_COMPRESS_CONFIG = {
    'çœ¼': {'max_files': 20, 'max_summary': 500},
    'explorer': {'max_files': 20, 'max_summary': 500},
    'é¼»': {'max_issues': 10, 'max_chars': 800},
    'reviewer': {'max_issues': 10, 'max_chars': 800},
    'æ–—æˆ˜èƒœä½›': {'max_chars': 2000, 'keep_diff_only': True},
    'impl': {'max_chars': 2000, 'keep_diff_only': True},
    'èº«': {'max_chars': 2000, 'keep_diff_only': True},
    'default': {'max_chars': 1000},
}


def compress_avatar_output(output: str, avatar_type: str) -> str:
    """
    æ ¹æ®åˆ†èº«ç±»å‹å‹ç¼©è¾“å‡ºå†…å®¹ã€‚

    å‹ç¼©ç­–ç•¥:
    - çœ¼åˆ†èº«: æœ€å¤š 20 ä¸ªæ–‡ä»¶ + 500 å­—æ‘˜è¦
    - é¼»åˆ†èº«: æœ€å¤š 10 ä¸ª issues
    - æ–—æˆ˜èƒœä½›: æœ€å¤š 2000 å­—ï¼Œåªä¿ç•™ diff æ‘˜è¦
    - å…¶ä»–åˆ†èº«: æœ€å¤š 1000 å­—

    Args:
        output: åˆ†èº«çš„åŸå§‹è¾“å‡º
        avatar_type: åˆ†èº«ç±»å‹ (çœ¼/è€³/é¼»/èˆŒ/èº«/æ„ æˆ–è‹±æ–‡åˆ«å)

    Returns:
        å‹ç¼©åçš„è¾“å‡º
    """
    config = AVATAR_COMPRESS_CONFIG.get(avatar_type, AVATAR_COMPRESS_CONFIG['default'])

    if avatar_type in ('çœ¼', 'explorer'):
        return _compress_explorer_output(output, config)
    elif avatar_type in ('é¼»', 'reviewer'):
        return _compress_reviewer_output(output, config)
    elif avatar_type in ('æ–—æˆ˜èƒœä½›', 'impl', 'èº«'):
        return _compress_impl_output(output, config)
    else:
        return _compress_generic_output(output, config)


def _compress_explorer_output(output: str, config: dict) -> str:
    """å‹ç¼©çœ¼åˆ†èº«è¾“å‡º: ä¿ç•™æ–‡ä»¶åˆ—è¡¨ + å‘ç°æ‘˜è¦"""
    max_files = config.get('max_files', 20)
    max_summary = config.get('max_summary', 500)

    lines = output.split('\n')
    compressed_lines = []

    # æå–æ–‡ä»¶è·¯å¾„
    file_paths = []
    file_pattern = re.compile(r'^[\s\-\*]*(/[^\s]+|\.{1,2}/[^\s]+|\w+/[^\s]+)')

    for line in lines:
        match = file_pattern.match(line.strip())
        if match:
            file_paths.append(match.group(1))

    # é™åˆ¶æ–‡ä»¶æ•°é‡
    if file_paths:
        compressed_lines.append("### å‘ç°çš„æ–‡ä»¶")
        for fp in file_paths[:max_files]:
            compressed_lines.append(f"- {fp}")
        if len(file_paths) > max_files:
            compressed_lines.append(f"- ... è¿˜æœ‰ {len(file_paths) - max_files} ä¸ªæ–‡ä»¶")

    # æå–ç»“è®º/å‘ç°éƒ¨åˆ†
    findings = _extract_findings(output)
    if findings:
        compressed_lines.append("")
        compressed_lines.append("### å‘ç°æ‘˜è¦")
        compressed_lines.append(findings[:max_summary])
        if len(findings) > max_summary:
            compressed_lines.append("...")

    return '\n'.join(compressed_lines) if compressed_lines else output[:max_summary]


def _compress_reviewer_output(output: str, config: dict) -> str:
    """å‹ç¼©é¼»åˆ†èº«è¾“å‡º: ä¿ç•™ issues åˆ—è¡¨"""
    max_issues = config.get('max_issues', 10)
    max_chars = config.get('max_chars', 800)

    # å°è¯•æå– issues
    issues = []
    issue_patterns = [
        r'(?:^|\n)[\s\-\*]*(?:issue|é—®é¢˜|warning|error|bug)[:\s]*(.+?)(?=\n[\s\-\*]*(?:issue|é—®é¢˜|warning|error|bug)|$)',
        r'(?:^|\n)\d+\.\s*(.+?)(?=\n\d+\.|$)',
        r'(?:^|\n)[\-\*]\s*(.+?)(?=\n[\-\*]|$)',
    ]

    for pattern in issue_patterns:
        matches = re.findall(pattern, output, re.IGNORECASE | re.DOTALL)
        if matches:
            issues.extend(matches)
            break

    if issues:
        compressed_lines = ["### å®¡æŸ¥é—®é¢˜"]
        for i, issue in enumerate(issues[:max_issues]):
            issue_text = issue.strip()[:150]
            compressed_lines.append(f"{i+1}. {issue_text}")
        if len(issues) > max_issues:
            compressed_lines.append(f"... è¿˜æœ‰ {len(issues) - max_issues} ä¸ªé—®é¢˜")
        return '\n'.join(compressed_lines)

    # å¦‚æœæ— æ³•æå– issuesï¼Œç›´æ¥æˆªæ–­
    return _compress_generic_output(output, config)


def _compress_impl_output(output: str, config: dict) -> str:
    """å‹ç¼©æ–—æˆ˜èƒœä½›è¾“å‡º: ä¿ç•™ diff æ‘˜è¦ï¼Œç§»é™¤å®Œæ•´ä»£ç """
    max_chars = config.get('max_chars', 2000)

    compressed_parts = []

    # 1. æå–ä¿®æ”¹æ‘˜è¦
    summary = _extract_change_summary(output)
    if summary:
        compressed_parts.append("### ä¿®æ”¹æ‘˜è¦")
        compressed_parts.append(summary)

    # 2. æå–æ–‡ä»¶å˜æ›´åˆ—è¡¨
    files_changed = _extract_files_changed(output)
    if files_changed:
        compressed_parts.append("")
        compressed_parts.append("### å˜æ›´æ–‡ä»¶")
        for f in files_changed[:10]:
            compressed_parts.append(f"- {f}")

    # 3. æå– diff æ¦‚è¦ (ä¸ä¿ç•™å®Œæ•´ä»£ç )
    diff_summary = _extract_diff_summary(output)
    if diff_summary:
        compressed_parts.append("")
        compressed_parts.append("### Diff æ¦‚è¦")
        compressed_parts.append(diff_summary[:800])

    # 4. æå–æ„å»º/æµ‹è¯•ç»“æœ
    test_result = _extract_test_result(output)
    if test_result:
        compressed_parts.append("")
        compressed_parts.append("### éªŒè¯ç»“æœ")
        compressed_parts.append(test_result)

    result = '\n'.join(compressed_parts)
    return result[:max_chars] if result else output[:max_chars]


def _compress_generic_output(output: str, config: dict) -> str:
    """é€šç”¨å‹ç¼©: æå–ç»“è®ºï¼Œé™åˆ¶å­—æ•°"""
    max_chars = config.get('max_chars', 1000)

    # å°è¯•æå–ç»“è®ºéƒ¨åˆ†
    conclusion = _extract_conclusion(output)
    if conclusion:
        return conclusion[:max_chars]

    # ç›´æ¥æˆªæ–­
    if len(output) > max_chars:
        return output[:max_chars] + "\n... [å·²æˆªæ–­]"
    return output


def _extract_findings(text: str) -> str:
    """æå–æ¢ç´¢å‘ç°"""
    patterns = [
        r'(?:å‘ç°|findings?|ç»“è®º|conclusion)[:\s]*(.+?)(?=\n\n|\Z)',
        r'(?:æ€»ç»“|summary)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()

    return ""


def _extract_change_summary(text: str) -> str:
    """æå–ä¿®æ”¹æ‘˜è¦"""
    patterns = [
        r'(?:ä¿®æ”¹æ‘˜è¦|changes?|summary)[:\s]*(.+?)(?=\n\n|\n###|\Z)',
        r'(?:å®Œæˆ|done|finished)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()[:500]

    return ""


def _extract_files_changed(text: str) -> list[str]:
    """æå–å˜æ›´çš„æ–‡ä»¶åˆ—è¡¨"""
    files = []

    # åŒ¹é…å¸¸è§çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
    file_patterns = [
        r'(?:modified|changed|created|deleted|edited)[:\s]*([^\n]+\.(?:py|js|ts|md|json|yaml|yml|toml))',
        r'(?:files?_changed|å˜æ›´æ–‡ä»¶)[:\s\[]*([^\]]+)',
        r'(?:^|\n)[\s\-\*]+(/[^\s]+\.[a-z]+)',
    ]

    for pattern in file_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        files.extend(matches)

    # å»é‡
    return list(dict.fromkeys(files))


def _extract_diff_summary(text: str) -> str:
    """æå– diff æ¦‚è¦ï¼Œç§»é™¤å®Œæ•´ä»£ç å—"""
    # ç§»é™¤ä»£ç å—
    text_no_code = re.sub(r'```[\s\S]*?```', '[ä»£ç å—å·²çœç•¥]', text)

    # æå– +/- è¡Œçš„ç»Ÿè®¡
    plus_lines = len(re.findall(r'^\+[^+]', text, re.MULTILINE))
    minus_lines = len(re.findall(r'^-[^-]', text, re.MULTILINE))

    summary_parts = []
    if plus_lines or minus_lines:
        summary_parts.append(f"+{plus_lines} -{minus_lines} è¡Œå˜æ›´")

    # æå–å‡½æ•°/ç±»å˜æ›´
    func_changes = re.findall(r'(?:def|class|function)\s+(\w+)', text_no_code)
    if func_changes:
        summary_parts.append(f"æ¶‰åŠ: {', '.join(set(func_changes)[:5])}")

    return ' | '.join(summary_parts) if summary_parts else ""


def _extract_test_result(text: str) -> str:
    """æå–æµ‹è¯•/æ„å»ºç»“æœ"""
    patterns = [
        r'((?:tests?\s+)?(?:passed|failed|success|error)[^\n]*)',
        r'((?:build|æ„å»º)\s*(?:æˆåŠŸ|å¤±è´¥|passed|failed)[^\n]*)',
        r'(âœ“|âœ—|PASS|FAIL)[^\n]*',
    ]

    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        results.extend(matches[:3])

    return ' | '.join(results) if results else ""


def _extract_conclusion(text: str) -> str:
    """æå–ç»“è®ºéƒ¨åˆ†"""
    patterns = [
        r'(?:ç»“è®º|conclusion|æ€»ç»“|summary|ç»“æœ|result)[:\s]*(.+?)(?=\n\n|\Z)',
        r'(?:å®Œæˆ|done|å®Œæ¯•)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()

    return ""


def compress_avatar_output_from_env(output: str) -> str:
    """
    ä»ç¯å¢ƒå˜é‡è¯»å–åˆ†èº«ç±»å‹å¹¶å‹ç¼©è¾“å‡ºã€‚

    ç¯å¢ƒå˜é‡:
    - WUKONG_COMPRESS_AVATAR: è®¾ä¸º 1 å¯ç”¨å‹ç¼©
    - WUKONG_AVATAR_TYPE: åˆ†èº«ç±»å‹
    """
    if os.environ.get('WUKONG_COMPRESS_AVATAR') != '1':
        return output

    avatar_type = os.environ.get('WUKONG_AVATAR_TYPE', 'default')
    return compress_avatar_output(output, avatar_type)


def read_hook_input() -> dict[str, Any]:
    """ä» stdin è¯»å– hook è¾“å…¥"""
    try:
        return json.load(sys.stdin)
    except json.JSONDecodeError:
        return {}


def read_transcript(transcript_path: str) -> list[dict]:
    """è¯»å–å¯¹è¯è®°å½•"""
    messages = []
    if not transcript_path:
        return messages

    path = Path(transcript_path).expanduser()
    if not path.exists() or not path.is_file():
        return messages

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    messages.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return messages


def extract_decisions(messages: list[dict]) -> list[dict]:
    """æå–å†³ç­–ä¿¡æ¯"""
    decisions = []
    decision_patterns = [
        r'\[D\d+\]',  # [D001] æ ¼å¼çš„å†³ç­–å¼•ç”¨
        r'å†³å®š|å†³ç­–|é€‰æ‹©|é‡‡ç”¨|ä½¿ç”¨',  # å†³ç­–å…³é”®è¯
        r'Decision|Decided|Choose|Use',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in decision_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                # æå–åŒ…å«å†³ç­–çš„æ®µè½
                decisions.append({
                    'type': 'decision',
                    'content': content[:500],  # é™åˆ¶é•¿åº¦
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return decisions[-5:]  # åªä¿ç•™æœ€è¿‘5ä¸ª


def extract_constraints(messages: list[dict]) -> list[dict]:
    """æå–çº¦æŸä¿¡æ¯"""
    constraints = []
    constraint_patterns = [
        r'\[C\d+\]',  # [C001] æ ¼å¼çš„çº¦æŸå¼•ç”¨
        r'å¿…é¡»|ç¦æ­¢|ä¸èƒ½|ä¸å…è®¸|çº¦æŸ|é™åˆ¶',
        r'MUST|NEVER|ALWAYS|constraint',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in constraint_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                constraints.append({
                    'type': 'constraint',
                    'content': content[:300],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return constraints[-3:]


def extract_interfaces(messages: list[dict]) -> list[dict]:
    """æå–æ¥å£å®šä¹‰"""
    interfaces = []
    interface_patterns = [
        r'\[I\d+\]',  # [I001] æ ¼å¼çš„æ¥å£å¼•ç”¨
        r'def \w+\(.*\)',  # Python å‡½æ•°å®šä¹‰
        r'class \w+',  # ç±»å®šä¹‰
        r'interface|API|endpoint',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in interface_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                interfaces.append({
                    'type': 'interface',
                    'content': content[:400],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return interfaces[-3:]


def extract_problems(messages: list[dict]) -> list[dict]:
    """æå–é—®é¢˜/é™·é˜±"""
    problems = []
    problem_patterns = [
        r'\[P\d+\]',  # [P001] æ ¼å¼çš„é—®é¢˜å¼•ç”¨
        r'é—®é¢˜|bug|é”™è¯¯|å¤±è´¥|è­¦å‘Š',
        r'error|fail|warning|issue|problem',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in problem_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                problems.append({
                    'type': 'problem',
                    'content': content[:300],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return problems[-3:]


def get_message_content(msg: dict) -> str:
    """è·å–æ¶ˆæ¯å†…å®¹"""
    if 'message' in msg and 'content' in msg['message']:
        content = msg['message']['content']
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return ' '.join(
                item.get('text', '')
                for item in content
                if isinstance(item, dict) and item.get('type') == 'text'
            )
    return ''


def extract_current_task(messages: list[dict]) -> str:
    """æå–å½“å‰ä»»åŠ¡æè¿°"""
    # æŸ¥æ‰¾ç”¨æˆ·çš„åˆå§‹è¯·æ±‚
    for msg in messages[:5]:  # åªçœ‹å‰å‡ æ¡
        if msg.get('type') == 'user':
            content = get_message_content(msg)
            if content:
                return content[:200]
    return "æœªçŸ¥ä»»åŠ¡"


def extract_progress(messages: list[dict]) -> dict:
    """æå–è¿›åº¦ä¿¡æ¯"""
    # ç®€å•ç»Ÿè®¡
    total_messages = len(messages)
    user_messages = sum(1 for m in messages if m.get('type') == 'user')
    assistant_messages = sum(1 for m in messages if m.get('type') == 'assistant')

    return {
        'total_turns': total_messages // 2,
        'user_messages': user_messages,
        'assistant_messages': assistant_messages,
    }


def generate_compact_context(
    task: str,
    decisions: list[dict],
    constraints: list[dict],
    interfaces: list[dict],
    problems: list[dict],
    progress: dict
) -> str:
    """ç”Ÿæˆç¼©å½¢æ€ä¸Šä¸‹æ–‡"""
    lines = [
        "## ğŸ”¸ ç¼©å½¢æ€ä¸Šä¸‹æ–‡",
        "",
        f"ã€ä»»åŠ¡ã€‘{task}",
        "",
        "ã€å·²å†³ç­–ã€‘",
    ]

    if decisions:
        for d in decisions[:3]:
            content = d['content'][:100].replace('\n', ' ')
            lines.append(f"- {content}...")
    else:
        lines.append("- (æš‚æ— )")

    lines.extend([
        "",
        "ã€çº¦æŸã€‘",
    ])

    if constraints:
        for c in constraints[:2]:
            content = c['content'][:80].replace('\n', ' ')
            lines.append(f"- {content}...")
    else:
        lines.append("- (æš‚æ— )")

    lines.extend([
        "",
        "ã€å½“å‰è¿›åº¦ã€‘",
        f"- å¯¹è¯è½®æ¬¡: {progress.get('total_turns', 0)}",
    ])

    if problems:
        lines.extend([
            "",
            "ã€æ³¨æ„äº‹é¡¹ã€‘",
        ])
        for p in problems[:2]:
            content = p['content'][:60].replace('\n', ' ')
            lines.append(f"- {content}...")

    lines.extend([
        "",
        f"ã€ç”Ÿæˆæ—¶é—´ã€‘{datetime.now().isoformat()}",
    ])

    return '\n'.join(lines)


def generate_anchor_candidates(
    decisions: list[dict],
    constraints: list[dict],
    problems: list[dict]
) -> list[dict]:
    """ç”Ÿæˆå€™é€‰é”šç‚¹ï¼ˆå¸¦å†…å®¹éªŒè¯ï¼‰"""
    candidates = []

    # å†³ç­–é”šç‚¹
    for i, d in enumerate(decisions):
        content = d['content']
        if not _is_valid_anchor_content(content, 'decision'):
            continue
        candidates.append({
            'id': f'D_candidate_{i}',
            'type': 'decision',
            'title': _extract_title(content, 'decision'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'decision'),
            }
        })

    # é—®é¢˜é”šç‚¹
    for i, p in enumerate(problems):
        content = p['content']
        if not _is_valid_anchor_content(content, 'problem'):
            continue
        candidates.append({
            'id': f'P_candidate_{i}',
            'type': 'problem',
            'title': _extract_title(content, 'problem'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'problem'),
            }
        })

    # çº¦æŸé”šç‚¹
    for i, c in enumerate(constraints):
        content = c['content']
        if not _is_valid_anchor_content(content, 'constraint'):
            continue
        candidates.append({
            'id': f'C_candidate_{i}',
            'type': 'constraint',
            'title': _extract_title(content, 'constraint'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'constraint'),
            }
        })

    return candidates


def _extract_title(content: str, anchor_type: str) -> str:
    """ä»å†…å®¹ä¸­æå–æ ‡é¢˜"""
    lines = content.split('\n')

    # 1. ä¼˜å…ˆæŸ¥æ‰¾ markdown æ ‡é¢˜æ ¼å¼
    for line in lines[:5]:
        line = line.strip()
        md_title = re.match(r'^#{1,3}\s+(.+)$', line)
        if md_title:
            title = md_title.group(1).strip()
            return title[:50] if len(title) <= 50 else title[:47] + '...'

        bold_title = re.match(r'^\*\*(.+?)\*\*', line)
        if bold_title:
            title = bold_title.group(1).strip()
            return title[:50] if len(title) <= 50 else title[:47] + '...'

    # 2. æ’é™¤å¯¹è¯å¼€å¤´ï¼Œæ‰¾ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„è¡Œ
    skip_prefixes = (
        'å®Œæˆ', 'å¥½çš„', 'å¥½ï¼Œ', 'æ˜¯çš„', 'æ²¡é—®é¢˜', 'å·²', 'æˆ‘',
        'Done', 'OK', 'Yes', 'I\'ll', 'I will', 'Let me', 'You are',
        'This', 'The ', 'Here', '```',
    )

    for line in lines[:10]:
        line = line.strip()
        if not line or len(line) < 5:
            continue
        if any(line.startswith(p) for p in skip_prefixes):
            continue
        line = re.sub(r'^[\-\*\d\.]+\s*', '', line)
        return line[:50] if len(line) <= 50 else line[:47] + '...'

    return f'{anchor_type}_untitled'


# ============================================================
# é”šç‚¹å†…å®¹éªŒè¯ (Anchor Content Validation)
# ============================================================

CONVERSATION_PREFIXES = (
    'å®Œæˆ', 'å¥½çš„', 'å¥½ï¼Œ', 'æ˜¯çš„', 'æ²¡é—®é¢˜', 'å·²ç»', 'æˆ‘æ¥', 'æˆ‘ä¼š', 'è®©æˆ‘',
    'ç°åœ¨', 'æ¥ä¸‹æ¥', 'é¦–å…ˆ', 'ç„¶å', 'æœ€å', 'æ€»ç»“',
    'Done', 'OK', 'Yes', 'I\'ll', 'I will', 'I\'m', 'Let me', 'Now',
    'You are', 'This command', 'Here is', 'Here are', '```',
)

ANCHOR_STRUCTURE_KEYWORDS = {
    'decision': ['context', 'decision', 'impact', 'evidence', 'èƒŒæ™¯', 'å†³ç­–', 'å½±å“', 'åŸå› '],
    'problem': ['ç—‡çŠ¶', 'æ ¹å› ', 'è§£å†³', 'é¢„é˜²', 'symptom', 'root cause', 'solution', 'prevention'],
    'constraint': ['çº¦æŸ', 'å¿…é¡»', 'ç¦æ­¢', 'must', 'never', 'always', 'constraint'],
}

IMPACT_KEYWORDS = [
    'æ¶æ„', 'å®‰å…¨', 'æ€§èƒ½', 'å¤šæ¨¡å—', 'å…¨å±€', 'æ ¸å¿ƒ', 'å…³é”®',
    'architecture', 'security', 'performance', 'global', 'core', 'critical',
]


def _is_valid_anchor_content(content: str, anchor_type: str) -> bool:
    """éªŒè¯å†…å®¹æ˜¯å¦é€‚åˆä½œä¸ºé”šç‚¹"""
    if not content or len(content.strip()) < 50:
        return False

    first_line = content.strip().split('\n')[0].strip()
    if any(first_line.startswith(p) for p in CONVERSATION_PREFIXES):
        return False

    keywords = ANCHOR_STRUCTURE_KEYWORDS.get(anchor_type, [])
    if keywords:
        content_lower = content.lower()
        if not any(kw.lower() in content_lower for kw in keywords):
            return False

    return True


def _detect_impact(content: str) -> bool:
    """æ£€æµ‹å†…å®¹æ˜¯å¦æ¶‰åŠé‡å¤§å½±å“"""
    content_lower = content.lower()
    return any(kw.lower() in content_lower for kw in IMPACT_KEYWORDS)


def _detect_reusable(content: str, anchor_type: str) -> bool:
    """æ£€æµ‹å†…å®¹æ˜¯å¦å¯å¤ç”¨"""
    if anchor_type == 'problem':
        return True
    reusable_keywords = ['æ¨¡å¼', 'é€šç”¨', 'æœ€ä½³å®è·µ', 'pattern', 'generic', 'best practice']
    return any(kw.lower() in content.lower() for kw in reusable_keywords)


# ============================================================
# æ…§â†’è¯† äº¤æ¥åè®® (Hui -> Shi Handoff Protocol)
# ============================================================

def generate_hui_output(
    session_id: str,
    project_path: str,
    task: str,
    decisions: list[dict],
    constraints: list[dict],
    interfaces: list[dict],
    problems: list[dict],
    progress: dict,
    compact_context: str,
    candidates: list[dict]
) -> dict:
    """
    ç”Ÿæˆæ…§æ¨¡å—çš„æ ‡å‡†åŒ–è¾“å‡ºï¼ˆJSON æ ¼å¼ï¼‰ã€‚

    è¿™æ˜¯æ…§â†’è¯†çš„äº¤æ¥åè®®ï¼Œå®šä¹‰äº†ä¸¤ä¸ªæ¨¡å—ä¹‹é—´çš„æ•°æ®å¥‘çº¦ã€‚

    Args:
        session_id: ä¼šè¯ID
        project_path: é¡¹ç›®è·¯å¾„
        task: å½“å‰ä»»åŠ¡æè¿°
        decisions: å†³ç­–åˆ—è¡¨
        constraints: çº¦æŸåˆ—è¡¨
        interfaces: æ¥å£åˆ—è¡¨
        problems: é—®é¢˜åˆ—è¡¨
        progress: è¿›åº¦ä¿¡æ¯
        compact_context: ç¼©å½¢æ€ä¸Šä¸‹æ–‡ (markdown)
        candidates: å€™é€‰é”šç‚¹åˆ—è¡¨

    Returns:
        æ ‡å‡†åŒ–çš„æ…§æ¨¡å—è¾“å‡º (dict)
    """
    return {
        "version": "1.0",
        "timestamp": datetime.now().isoformat(),
        "session_id": session_id,
        "project_path": project_path,

        "context": {
            "task": task,
            "decisions": decisions,
            "constraints": constraints,
            "interfaces": interfaces,
            "problems": problems,
            "progress": progress,
            "compact_md": compact_context,
        },

        "anchors": candidates,  # å€™é€‰é”šç‚¹åˆ—è¡¨
    }


# ============================================================
# è¯†æ¨¡å— - å†™å…¥é€»è¾‘ (Shi Module - Write Logic)
# ============================================================

def check_threshold(anchor: dict) -> bool:
    """
    æ£€æŸ¥é”šç‚¹æ˜¯å¦é€šè¿‡å†™å…¥é—¨æ§›ï¼ˆä¸¥æ ¼ç‰ˆï¼‰ã€‚

    é—¨æ§›æ¡ä»¶ (è‡³å°‘æ»¡è¶³ä¸€é¡¹):
    - frequency >= 2: ç±»ä¼¼é—®é¢˜/å†³ç­–å‡ºç°ä¸¤æ¬¡ä»¥ä¸Š
    - impact = True AND å†…å®¹é•¿åº¦ >= 100
    - reusable = True AND anchor_type == 'problem' AND æœ‰è§£å†³æ–¹æ¡ˆ

    Args:
        anchor: é”šç‚¹å­—å…¸ï¼ŒåŒ…å« threshold_check å­—æ®µ

    Returns:
        bool: æ˜¯å¦é€šè¿‡é—¨æ§›
    """
    threshold = anchor.get('threshold_check', {})
    anchor_type = anchor.get('type', '')
    content = anchor.get('content', '')

    # 1. é¢‘ç‡æ£€æŸ¥ (æœ€å¯é )
    frequency = threshold.get('frequency', 0)
    if isinstance(frequency, int) and frequency >= 2:
        return True

    # 2. å½±å“æ£€æŸ¥ (éœ€é…åˆå†…å®¹é•¿åº¦)
    if threshold.get('impact', False) and len(content) >= 100:
        return True

    # 3. å¯å¤ç”¨æ£€æŸ¥ (åªå¯¹ problem ç±»å‹ + æœ‰è§£å†³æ–¹æ¡ˆç”Ÿæ•ˆ)
    if threshold.get('reusable', False) and anchor_type == 'problem':
        content_lower = content.lower()
        has_solution = any(kw in content_lower for kw in ['è§£å†³', 'ä¿®å¤', 'é¢„é˜²', 'fix', 'solution', 'resolve'])
        if has_solution:
            return True

    return False


def check_duplicate(anchor: dict, existing_anchors: list[dict]) -> tuple[bool, str | None]:
    """
    æ£€æŸ¥é”šç‚¹æ˜¯å¦ä¸ç°æœ‰é”šç‚¹é‡å¤ã€‚

    ä½¿ç”¨ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥:
    - æ ‡é¢˜å®Œå…¨ç›¸åŒ -> é‡å¤
    - æ ‡é¢˜è¯æ±‡é‡å  > 70% -> é‡å¤

    Args:
        anchor: å¾…æ£€æŸ¥çš„é”šç‚¹
        existing_anchors: ç°æœ‰é”šç‚¹åˆ—è¡¨

    Returns:
        (is_duplicate, existing_id): æ˜¯å¦é‡å¤åŠé‡å¤é”šç‚¹çš„ID
    """
    new_title = anchor.get('title', '').lower()
    new_words = set(re.findall(r'\w+', new_title))

    if not new_words:
        return False, None

    for existing in existing_anchors:
        existing_title = existing.get('title', '').lower()
        existing_words = set(re.findall(r'\w+', existing_title))

        if not existing_words:
            continue

        # å®Œå…¨ç›¸åŒ
        if new_title == existing_title:
            return True, existing.get('id')

        # è¯æ±‡é‡å æ£€æŸ¥
        intersection = new_words & existing_words
        union = new_words | existing_words

        if union and len(intersection) / len(union) > 0.7:
            return True, existing.get('id')

    return False, None


def _get_next_anchor_id(anchor_type: str, existing_anchors: list[dict]) -> str:
    """
    è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„é”šç‚¹IDã€‚

    IDæ ¼å¼: {type_prefix}{3ä½æ•°å­—}
    - å†³ç­–: D001, D002, ...
    - é—®é¢˜: P001, P002, ...
    - çº¦æŸ: C001, C002, ...
    - æ¥å£: I001, I002, ...

    Args:
        anchor_type: é”šç‚¹ç±»å‹ (decision, problem, constraint, interface)
        existing_anchors: ç°æœ‰é”šç‚¹åˆ—è¡¨

    Returns:
        æ–°çš„é”šç‚¹ID
    """
    type_prefix_map = {
        'decision': 'D',
        'problem': 'P',
        'constraint': 'C',
        'interface': 'I',
    }
    prefix = type_prefix_map.get(anchor_type, 'A')

    # æ‰¾å‡ºåŒç±»å‹çš„æœ€å¤§ID
    max_num = 0
    pattern = re.compile(rf'^{prefix}(\d+)$')

    for anchor in existing_anchors:
        anchor_id = anchor.get('id', '')
        match = pattern.match(anchor_id)
        if match:
            num = int(match.group(1))
            max_num = max(max_num, num)

    return f'{prefix}{max_num + 1:03d}'


def _load_existing_anchors(anchors_path: Path) -> list[dict]:
    """
    ä» anchors.md æ–‡ä»¶åŠ è½½ç°æœ‰é”šç‚¹ã€‚

    è§£ææ ¼å¼:
    ## [D001] å†³ç­–æ ‡é¢˜
    å†…å®¹...

    Args:
        anchors_path: anchors.md æ–‡ä»¶è·¯å¾„

    Returns:
        é”šç‚¹åˆ—è¡¨
    """
    if not anchors_path.exists():
        return []

    anchors = []
    content = anchors_path.read_text(encoding='utf-8')

    # åŒ¹é… ## [ID] æ ‡é¢˜ æ ¼å¼
    anchor_pattern = re.compile(
        r'^## \[([A-Z]\d+)\] (.+?)$\n(.*?)(?=^## \[|$)',
        re.MULTILINE | re.DOTALL
    )

    for match in anchor_pattern.finditer(content):
        anchor_id = match.group(1)
        title = match.group(2).strip()
        body = match.group(3).strip()

        # æ¨æ–­ç±»å‹
        anchor_type = 'unknown'
        if anchor_id.startswith('D'):
            anchor_type = 'decision'
        elif anchor_id.startswith('P'):
            anchor_type = 'problem'
        elif anchor_id.startswith('C'):
            anchor_type = 'constraint'
        elif anchor_id.startswith('I'):
            anchor_type = 'interface'

        anchors.append({
            'id': anchor_id,
            'type': anchor_type,
            'title': title,
            'content': body,
        })

    return anchors


def write_to_anchors(anchor: dict, anchors_path: Path) -> str:
    """
    å°†é”šç‚¹è¿½åŠ åˆ° anchors.md æ–‡ä»¶ã€‚

    Args:
        anchor: é”šç‚¹å­—å…¸
        anchors_path: anchors.md æ–‡ä»¶è·¯å¾„

    Returns:
        æ–°åˆ†é…çš„é”šç‚¹ID
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    anchors_path.parent.mkdir(parents=True, exist_ok=True)

    # åŠ è½½ç°æœ‰é”šç‚¹
    existing_anchors = _load_existing_anchors(anchors_path)

    # åˆ†é…æ–°ID
    new_id = _get_next_anchor_id(anchor.get('type', 'unknown'), existing_anchors)

    # æ„å»ºé”šç‚¹å†…å®¹
    title = anchor.get('title', 'Untitled')
    content = anchor.get('content', '')
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')

    anchor_entry = f"""
## [{new_id}] {title}

**åˆ›å»ºæ—¶é—´**: {timestamp}
**ç±»å‹**: {anchor.get('type', 'unknown')}

{content}

---
"""

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºå¤´éƒ¨
    if not anchors_path.exists():
        header = """# é”šç‚¹è®°å½• (Anchors)

> æ­¤æ–‡ä»¶ç”±è¯†æ¨¡å—è‡ªåŠ¨ç»´æŠ¤ï¼Œè®°å½•è·¨ä¼šè¯çš„é‡è¦å†³ç­–ã€é—®é¢˜å’Œçº¦æŸã€‚

"""
        with open(anchors_path, 'w', encoding='utf-8') as f:
            f.write(header)

    # è¿½åŠ é”šç‚¹
    with open(anchors_path, 'a', encoding='utf-8') as f:
        f.write(anchor_entry)

    return new_id


def _get_project_hash(project_path: str) -> str:
    """è®¡ç®—é¡¹ç›®è·¯å¾„çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç´¢å¼•ï¼‰"""
    return hashlib.md5(project_path.encode()).hexdigest()[:8]


def _get_project_name(project_path: str) -> str:
    """ä»é¡¹ç›®è·¯å¾„æå–é¡¹ç›®å"""
    return Path(project_path).name or 'unknown'


def update_session_index(session_info: dict, index_path: Path):
    """
    æ›´æ–°ä¼šè¯ç´¢å¼•æ–‡ä»¶ (index.json)ã€‚

    ç´¢å¼•ç»“æ„:
    {
        "version": "1.0",
        "sessions": [...],
        "projects": {...}
    }

    Args:
        session_info: ä¼šè¯ä¿¡æ¯å­—å…¸
        index_path: index.json æ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    index_path.parent.mkdir(parents=True, exist_ok=True)

    # åŠ è½½ç°æœ‰ç´¢å¼•
    index = {
        "version": "1.0",
        "sessions": [],
        "projects": {}
    }

    if index_path.exists():
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                index = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    # ç¡®ä¿ç»“æ„å®Œæ•´
    if "sessions" not in index:
        index["sessions"] = []
    if "projects" not in index:
        index["projects"] = {}

    session_id = session_info.get('session_id', 'unknown')
    project_path = session_info.get('project_path', '.')
    project_hash = _get_project_hash(project_path)
    project_name = _get_project_name(project_path)
    now = datetime.now().isoformat()

    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ä¼šè¯
    existing_session = None
    for i, s in enumerate(index["sessions"]):
        if s.get('session_id') == session_id:
            existing_session = i
            break

    session_entry = {
        "session_id": session_id,
        "project_path": project_path,
        "project_hash": project_hash,
        "created_at": session_info.get('created_at', now),
        "updated_at": now,
        "task_summary": session_info.get('task_summary', '')[:200],
        "anchor_count": session_info.get('anchor_count', 0),
        "status": session_info.get('status', 'active')
    }

    if existing_session is not None:
        # æ›´æ–°ç°æœ‰ä¼šè¯
        session_entry["created_at"] = index["sessions"][existing_session].get('created_at', now)
        index["sessions"][existing_session] = session_entry
    else:
        # æ·»åŠ æ–°ä¼šè¯
        index["sessions"].append(session_entry)

    # æ›´æ–°é¡¹ç›®ä¿¡æ¯
    if project_hash not in index["projects"]:
        index["projects"][project_hash] = {
            "path": project_path,
            "name": project_name,
            "session_count": 0
        }

    # ç»Ÿè®¡è¯¥é¡¹ç›®çš„ä¼šè¯æ•°
    project_sessions = sum(
        1 for s in index["sessions"]
        if s.get('project_hash') == project_hash
    )
    index["projects"][project_hash]["session_count"] = project_sessions

    # ä¿å­˜ç´¢å¼•
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def shi_write(hui_output: dict, cwd: str) -> dict:
    """
    è¯†æ¨¡å—çš„ä¸»å†™å…¥å‡½æ•°ã€‚

    æ¥æ”¶æ…§æ¨¡å—çš„ JSON è¾“å‡ºï¼Œæ‰§è¡Œ:
    1. é—¨æ§›æ£€æŸ¥
    2. å»é‡æ£€æŸ¥
    3. å†™å…¥é€šè¿‡çš„é”šç‚¹ï¼ˆåˆ°ç”¨æˆ·çº§åˆ«é¡¹ç›®é”šç‚¹æ–‡ä»¶ï¼‰
    4. æ›´æ–°ç´¢å¼•

    Args:
        hui_output: æ…§æ¨¡å—çš„æ ‡å‡†åŒ–è¾“å‡º
        cwd: å½“å‰å·¥ä½œç›®å½•

    Returns:
        å†™å…¥ç»“æœæ‘˜è¦
    """
    result = {
        "anchors_written": [],
        "anchors_skipped": [],
        "anchors_duplicated": [],
        "errors": []
    }

    # å‡†å¤‡è·¯å¾„ï¼ˆç”¨æˆ·çº§åˆ«ï¼‰
    anchors_path = get_project_anchors_path(cwd)
    index_path = get_session_index_path()

    # åŠ è½½ç°æœ‰é”šç‚¹
    existing_anchors = _load_existing_anchors(anchors_path)

    # å¤„ç†å€™é€‰é”šç‚¹
    candidates = hui_output.get('anchors', [])

    for candidate in candidates:
        try:
            # 1. é—¨æ§›æ£€æŸ¥
            if not check_threshold(candidate):
                result["anchors_skipped"].append({
                    "id": candidate.get('id'),
                    "reason": "threshold_not_met"
                })
                continue

            # 2. å»é‡æ£€æŸ¥
            is_dup, existing_id = check_duplicate(candidate, existing_anchors)
            if is_dup:
                result["anchors_duplicated"].append({
                    "id": candidate.get('id'),
                    "existing_id": existing_id
                })
                continue

            # 3. å†™å…¥é”šç‚¹
            new_id = write_to_anchors(candidate, anchors_path)
            result["anchors_written"].append({
                "candidate_id": candidate.get('id'),
                "new_id": new_id,
                "type": candidate.get('type'),
                "title": candidate.get('title')
            })

            # æ›´æ–°ç°æœ‰é”šç‚¹åˆ—è¡¨ï¼ˆç”¨äºåç»­å»é‡ï¼‰
            existing_anchors.append({
                'id': new_id,
                'type': candidate.get('type'),
                'title': candidate.get('title'),
                'content': candidate.get('content')
            })

        except Exception as e:
            result["errors"].append({
                "id": candidate.get('id'),
                "error": str(e)
            })

    # 4. æ›´æ–°ä¼šè¯ç´¢å¼•
    try:
        session_info = {
            "session_id": hui_output.get('session_id', 'unknown'),
            "project_path": hui_output.get('project_path', cwd),
            "task_summary": hui_output.get('context', {}).get('task', ''),
            "anchor_count": len(result["anchors_written"]),
            "status": "active"
        }
        update_session_index(session_info, index_path)
    except Exception as e:
        result["errors"].append({
            "id": "session_index",
            "error": str(e)
        })

    return result


# ============================================================
# è¯†æ¨¡å— - æƒ¯æ€§æç¤º API (Shi Module - Inertia Prompt API)
# ============================================================

# åˆ†èº«ç±»å‹æ˜ å°„ (ç”¨äºè§„èŒƒåŒ–åˆ†èº«åç§°)
AVATAR_TYPE_MAP = {
    # çœ¼åˆ†èº«
    'çœ¼': 'eye', 'explorer': 'eye', 'eye': 'eye',
    # è€³åˆ†èº«
    'è€³': 'ear', 'analyst': 'ear', 'ear': 'ear',
    # é¼»åˆ†èº«
    'é¼»': 'nose', 'reviewer': 'nose', 'nose': 'nose',
    # èˆŒåˆ†èº«
    'èˆŒ': 'tongue', 'tester': 'tongue', 'tongue': 'tongue',
    # èº«åˆ†èº« (æ–—æˆ˜èƒœä½›)
    'èº«': 'body', 'æ–—æˆ˜èƒœä½›': 'body', 'impl': 'body', 'implementer': 'body', 'body': 'body',
    # æ„åˆ†èº«
    'æ„': 'mind', 'architect': 'mind', 'mind': 'mind',
}

# åˆ†èº«æƒ¯æ€§æç¤ºé…ç½®
# T1: ä»»åŠ¡å¯åŠ¨å‰ (P+C+M)
# T2: æ–¹æ¡ˆå†»ç»“å (D+I)
AVATAR_INERTIA_CONFIG = {
    'eye': {'t1': True, 't2': False},      # çœ¼: æ¢ç´¢å‰æç¤ºå·²çŸ¥é—®é¢˜
    'ear': {'t1': False, 't2': False},     # è€³: éœ€æ±‚åˆ†ææ— éœ€å†å²åŒ…è¢±
    'nose': {'t1': True, 't2': True},      # é¼»: å®¡æŸ¥æ—¶å‚è€ƒçº¦æŸå’Œå†³ç­–
    'tongue': {'t1': True, 't2': False},   # èˆŒ: æµ‹è¯•æ—¶æç¤ºå·²çŸ¥é™·é˜±
    'body': {'t1': True, 't2': True},      # èº«: å®ç°å‰è·å–å®Œæ•´ä¸Šä¸‹æ–‡
    'mind': {'t1': False, 't2': True},     # æ„: è®¾è®¡æ—¶å‚è€ƒå†å²å†³ç­–
}


def _normalize_avatar_type(avatar_type: str) -> str:
    """è§„èŒƒåŒ–åˆ†èº«ç±»å‹åç§°"""
    return AVATAR_TYPE_MAP.get(avatar_type.lower(), 'unknown')


def _filter_anchors_by_keywords(
    anchors: list[dict],
    keywords: list[str] | None = None
) -> list[dict]:
    """
    æ ¹æ®å…³é”®è¯è¿‡æ»¤é”šç‚¹ã€‚

    å¦‚æœ keywords ä¸ºç©ºï¼Œè¿”å›æ‰€æœ‰é”šç‚¹ï¼ˆæœ€å¤š 5 ä¸ªï¼‰ã€‚
    å¦åˆ™è¿”å›æ ‡é¢˜æˆ–å†…å®¹åŒ…å«å…³é”®è¯çš„é”šç‚¹ã€‚

    Args:
        anchors: é”šç‚¹åˆ—è¡¨
        keywords: è¿‡æ»¤å…³é”®è¯

    Returns:
        è¿‡æ»¤åçš„é”šç‚¹åˆ—è¡¨
    """
    if not anchors:
        return []

    if not keywords:
        # æ— å…³é”®è¯ï¼Œè¿”å›æœ€è¿‘çš„é”šç‚¹ï¼ˆæœ€å¤š 5 ä¸ªï¼‰
        return anchors[-5:] if len(anchors) > 5 else anchors

    # æŒ‰å…³é”®è¯è¿‡æ»¤
    matched = []
    keywords_lower = [k.lower() for k in keywords]

    for anchor in anchors:
        title = anchor.get('title', '').lower()
        content = anchor.get('content', '').lower()

        for kw in keywords_lower:
            if kw in title or kw in content:
                matched.append(anchor)
                break

    return matched[-5:] if len(matched) > 5 else matched


def _format_anchor_for_prompt(anchor: dict) -> str:
    """æ ¼å¼åŒ–å•ä¸ªé”šç‚¹ç”¨äºæç¤º"""
    anchor_id = anchor.get('id', 'A000')
    title = anchor.get('title', '')[:50]
    content = anchor.get('content', '').strip()

    # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œåªæ˜¾ç¤ºæ ‡é¢˜
    if not content:
        return f"[{anchor_id}] {title}"

    # æå–å†…å®¹çš„ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„æ–‡å­—
    content_preview = ''
    for line in content.split('\n'):
        line = line.strip()
        # è·³è¿‡å…ƒæ•°æ®è¡Œå’Œåˆ†éš”ç¬¦
        if not line or line.startswith('**') or line.startswith('---') or line.startswith('#'):
            continue
        content_preview = line[:80]
        break

    if content_preview:
        return f"[{anchor_id}] {title}: {content_preview}"
    return f"[{anchor_id}] {title}"


def get_shi_t1_prompt(cwd: str, keywords: list[str] = None) -> str:
    """
    T1 æƒ¯æ€§æç¤º (ä»»åŠ¡å¯åŠ¨å‰)ã€‚

    æŸ¥è¯¢ç±»å‹: P(é—®é¢˜) + C(çº¦æŸ) + M(æ¨¡å¼)
    é€‚ç”¨åˆ†èº«: çœ¼ã€èº«ã€èˆŒã€é¼»

    Args:
        cwd: å½“å‰å·¥ä½œç›®å½•
        keywords: å¯é€‰çš„å…³é”®è¯è¿‡æ»¤

    Returns:
        æ ¼å¼åŒ–çš„ T1 æƒ¯æ€§æç¤º
    """
    anchors_path = get_project_anchors_path(cwd)
    all_anchors = _load_existing_anchors(anchors_path)

    if not all_anchors:
        return """## [è¯† T1] å¯åŠ¨æç¤º

æš‚æ— ç›¸å…³é”šç‚¹ã€‚

---
> ä»…ä¾›å‚è€ƒï¼Œä¸å½±å“å†³ç­–"""

    # æŒ‰ç±»å‹åˆ†ç±»é”šç‚¹
    problems = [a for a in all_anchors if a.get('type') == 'problem']
    constraints = [a for a in all_anchors if a.get('type') == 'constraint']
    patterns = [a for a in all_anchors if 'pattern' in a.get('type', '') or
                'æ¨¡å¼' in a.get('title', '') or 'pattern' in a.get('title', '').lower()]

    # åº”ç”¨å…³é”®è¯è¿‡æ»¤
    if keywords:
        problems = _filter_anchors_by_keywords(problems, keywords)
        constraints = _filter_anchors_by_keywords(constraints, keywords)
        patterns = _filter_anchors_by_keywords(patterns, keywords)
    else:
        # æ— å…³é”®è¯æ—¶ï¼Œæ¯ç±»æœ€å¤š 3 ä¸ª
        problems = problems[-3:] if len(problems) > 3 else problems
        constraints = constraints[-3:] if len(constraints) > 3 else constraints
        patterns = patterns[-3:] if len(patterns) > 3 else patterns

    lines = ["## [è¯† T1] å¯åŠ¨æç¤º", ""]

    # ç›¸å…³é£é™© (Problems)
    if problems:
        lines.append("**ç›¸å…³é£é™©**:")
        for p in problems:
            lines.append(f"- {_format_anchor_for_prompt(p)}")
        lines.append("")

    # çº¦æŸæé†’ (Constraints)
    if constraints:
        lines.append("**çº¦æŸæé†’**:")
        for c in constraints:
            lines.append(f"- {_format_anchor_for_prompt(c)}")
        lines.append("")

    # å¯å¤ç”¨æ¨¡å¼ (Patterns)
    if patterns:
        lines.append("**å¯å¤ç”¨æ¨¡å¼**:")
        for m in patterns:
            lines.append(f"- {_format_anchor_for_prompt(m)}")
        lines.append("")

    # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…
    if not problems and not constraints and not patterns:
        lines.append("æš‚æ— ä¸å½“å‰ä»»åŠ¡ç›¸å…³çš„é”šç‚¹ã€‚")
        lines.append("")

    lines.extend([
        "---",
        "> ä»…ä¾›å‚è€ƒï¼Œä¸å½±å“å†³ç­–"
    ])

    return '\n'.join(lines)


def get_shi_t2_prompt(cwd: str, keywords: list[str] = None) -> str:
    """
    T2 æƒ¯æ€§æç¤º (æ–¹æ¡ˆå†»ç»“å)ã€‚

    æŸ¥è¯¢ç±»å‹: D(å†³ç­–) + I(æ¥å£)
    é€‚ç”¨åˆ†èº«: æ„ã€èº«ã€é¼»

    Args:
        cwd: å½“å‰å·¥ä½œç›®å½•
        keywords: å¯é€‰çš„å…³é”®è¯è¿‡æ»¤

    Returns:
        æ ¼å¼åŒ–çš„ T2 æƒ¯æ€§æç¤º
    """
    anchors_path = get_project_anchors_path(cwd)
    all_anchors = _load_existing_anchors(anchors_path)

    if not all_anchors:
        return """## [è¯† T2] è®¾è®¡å‚è€ƒ

æš‚æ— ç›¸å…³é”šç‚¹ã€‚

---
> ä»…ä¾›å‚è€ƒï¼Œå†³ç­–æƒåœ¨æœ¬ä½“"""

    # æŒ‰ç±»å‹åˆ†ç±»é”šç‚¹
    decisions = [a for a in all_anchors if a.get('type') == 'decision']
    interfaces = [a for a in all_anchors if a.get('type') == 'interface']

    # åº”ç”¨å…³é”®è¯è¿‡æ»¤
    if keywords:
        decisions = _filter_anchors_by_keywords(decisions, keywords)
        interfaces = _filter_anchors_by_keywords(interfaces, keywords)
    else:
        # æ— å…³é”®è¯æ—¶ï¼Œæ¯ç±»æœ€å¤š 5 ä¸ª
        decisions = decisions[-5:] if len(decisions) > 5 else decisions
        interfaces = interfaces[-5:] if len(interfaces) > 5 else interfaces

    lines = ["## [è¯† T2] è®¾è®¡å‚è€ƒ", ""]

    # å†å²å†³ç­– (Decisions)
    if decisions:
        lines.append("**å†å²å†³ç­–**:")
        lines.append("| ID | å†³ç­– | ç†ç”± |")
        lines.append("|----|------|------|")
        for d in decisions:
            anchor_id = d.get('id', 'D000')
            title = d.get('title', '')[:30]
            # å°è¯•ä»å†…å®¹ä¸­æå–ç†ç”±
            content = d.get('content', '')
            reason = ''
            if 'ç†ç”±' in content or 'reason' in content.lower():
                # ç®€å•æå–ç†ç”±
                for line in content.split('\n'):
                    if 'ç†ç”±' in line or 'reason' in line.lower():
                        reason = line.split(':', 1)[-1].strip()[:30]
                        break
            if not reason:
                reason = content[:30].replace('\n', ' ')
            lines.append(f"| [{anchor_id}] | {title} | {reason} |")
        lines.append("")

    # ç›¸å…³æ¥å£ (Interfaces)
    if interfaces:
        lines.append("**ç›¸å…³æ¥å£**:")
        for i in interfaces:
            lines.append(f"- {_format_anchor_for_prompt(i)}")
        lines.append("")

    # å›æ»šç»éªŒ (ä»å†³ç­–ä¸­æå–)
    rollback_info = []
    for d in decisions:
        content = d.get('content', '').lower()
        if 'å›æ»š' in content or 'rollback' in content or 'æ¢å¤' in content:
            anchor_id = d.get('id', 'D000')
            # å°è¯•æå–å›æ»šå‘½ä»¤
            for line in d.get('content', '').split('\n'):
                if '`' in line:
                    # æå–åå¼•å·ä¸­çš„å‘½ä»¤
                    import re
                    cmds = re.findall(r'`([^`]+)`', line)
                    if cmds:
                        rollback_info.append(f"{anchor_id}: `{cmds[0]}`")
                        break

    if rollback_info:
        lines.append("**å›æ»šç»éªŒ**:")
        for r in rollback_info[:3]:
            lines.append(f"- {r}")
        lines.append("")

    # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…
    if not decisions and not interfaces:
        lines.append("æš‚æ— ä¸å½“å‰è®¾è®¡ç›¸å…³çš„é”šç‚¹ã€‚")
        lines.append("")

    lines.extend([
        "---",
        "> ä»…ä¾›å‚è€ƒï¼Œå†³ç­–æƒåœ¨æœ¬ä½“"
    ])

    return '\n'.join(lines)


def get_shi_prompt_for_avatar(
    cwd: str,
    avatar_type: str,
    task_desc: str = ''
) -> str:
    """
    æ ¹æ®åˆ†èº«ç±»å‹è‡ªåŠ¨é€‰æ‹© T1/T2 æƒ¯æ€§æç¤ºã€‚

    åˆ†èº«é…ç½®:
    | åˆ†èº« | T1 | T2 | è¯´æ˜ |
    |------|----|----|------|
    | çœ¼/explorer | âœ“ | - | æ¢ç´¢å‰æç¤ºå·²çŸ¥é—®é¢˜ |
    | è€³/analyst | - | - | éœ€æ±‚åˆ†ææ— éœ€å†å²åŒ…è¢± |
    | æ„/architect | - | âœ“ | è®¾è®¡æ—¶å‚è€ƒå†å²å†³ç­– |
    | èº«/æ–—æˆ˜èƒœä½›/impl | âœ“ | âœ“ | å®ç°å‰è·å–å®Œæ•´ä¸Šä¸‹æ–‡ |
    | èˆŒ/tester | âœ“ | - | æµ‹è¯•æ—¶æç¤ºå·²çŸ¥é™·é˜± |
    | é¼»/reviewer | âœ“ | âœ“ | å®¡æŸ¥æ—¶å‚è€ƒçº¦æŸå’Œå†³ç­– |

    Args:
        cwd: å½“å‰å·¥ä½œç›®å½•
        avatar_type: åˆ†èº«ç±»å‹
        task_desc: å¯é€‰çš„ä»»åŠ¡æè¿°ï¼ˆç”¨äºæå–å…³é”®è¯ï¼‰

    Returns:
        åˆå¹¶çš„æƒ¯æ€§æç¤ºå­—ç¬¦ä¸²
    """
    # è§„èŒƒåŒ–åˆ†èº«ç±»å‹
    normalized_type = _normalize_avatar_type(avatar_type)

    if normalized_type == 'unknown':
        return ""

    # è·å–é…ç½®
    config = AVATAR_INERTIA_CONFIG.get(normalized_type, {'t1': False, 't2': False})

    # å¦‚æœéƒ½ä¸éœ€è¦
    if not config['t1'] and not config['t2']:
        return ""

    # ä»ä»»åŠ¡æè¿°ä¸­æå–å…³é”®è¯
    keywords = None
    if task_desc:
        # ç®€å•çš„å…³é”®è¯æå–ï¼šåˆ†è¯å¹¶è¿‡æ»¤å¸¸è§è¯
        import re
        words = re.findall(r'[\w\u4e00-\u9fff]+', task_desc)
        # è¿‡æ»¤çŸ­è¯å’Œå¸¸è§è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'æœ‰', 'è¿™', 'ä¸ª', 'è¦', 'ä¼š',
                      'the', 'a', 'an', 'is', 'are', 'to', 'and', 'or', 'for'}
        keywords = [w for w in words if len(w) > 1 and w.lower() not in stop_words]
        keywords = keywords[:5]  # æœ€å¤š 5 ä¸ªå…³é”®è¯

    prompts = []

    # è·å– T1 (å¦‚æœéœ€è¦)
    if config['t1']:
        t1_prompt = get_shi_t1_prompt(cwd, keywords)
        if t1_prompt and 'æš‚æ— ç›¸å…³é”šç‚¹' not in t1_prompt:
            prompts.append(t1_prompt)

    # è·å– T2 (å¦‚æœéœ€è¦)
    if config['t2']:
        t2_prompt = get_shi_t2_prompt(cwd, keywords)
        if t2_prompt and 'æš‚æ— ç›¸å…³é”šç‚¹' not in t2_prompt:
            prompts.append(t2_prompt)

    if not prompts:
        return ""

    return '\n\n'.join(prompts)


# ============================================================
# è·¨ä¼šè¯å†…è§‚ API (Cross-Session Introspection API)
# ============================================================

def _parse_date(date_str: str) -> datetime:
    """
    è§£ææ—¥æœŸå­—ç¬¦ä¸²ã€‚

    Args:
        date_str: 'today', 'yesterday', '2026-01-13' ç­‰æ ¼å¼

    Returns:
        datetime å¯¹è±¡
    """
    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

    if date_str == 'today':
        return today
    elif date_str == 'yesterday':
        return today - timedelta(days=1)
    elif date_str == 'week':
        # è¿”å›æœ¬å‘¨ä¸€
        return today - timedelta(days=today.weekday())
    else:
        # å°è¯•è§£æå…·ä½“æ—¥æœŸ
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except ValueError:
            return today


def _load_session_index() -> dict:
    """åŠ è½½ä¼šè¯ç´¢å¼•æ–‡ä»¶"""
    index_path = get_session_index_path()
    if not index_path.exists():
        return {"version": "1.0", "sessions": [], "projects": {}}

    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {"version": "1.0", "sessions": [], "projects": {}}


def query_sessions_by_date(date: str = 'today', project: str = None) -> list[dict]:
    """
    æŒ‰æ—¥æœŸæŸ¥è¯¢ä¼šè¯ã€‚

    Args:
        date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒ 'today', 'yesterday', '2026-01-13' æ ¼å¼
        project: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šé¡¹ç›®

    Returns:
        åŒ¹é…çš„ä¼šè¯åˆ—è¡¨
    """
    index = _load_session_index()
    sessions = index.get('sessions', [])

    target_date = _parse_date(date)

    # æ ¹æ® date ç±»å‹ç¡®å®šæ—¶é—´èŒƒå›´
    if date == 'week':
        end_date = target_date + timedelta(days=7)
    else:
        end_date = target_date + timedelta(days=1)

    matched = []
    for session in sessions:
        # è§£æä¼šè¯çš„åˆ›å»º/æ›´æ–°æ—¶é—´
        session_time_str = session.get('updated_at') or session.get('created_at', '')
        if not session_time_str:
            continue

        try:
            session_time = datetime.fromisoformat(session_time_str)
        except ValueError:
            continue

        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if not (target_date <= session_time < end_date):
            continue

        # é¡¹ç›®è¿‡æ»¤
        if project:
            session_project = session.get('project_path', '')
            project_name = Path(session_project).name if session_project else ''
            if project.lower() not in session_project.lower() and project.lower() not in project_name.lower():
                continue

        matched.append(session)

    return matched


def generate_daily_summary(date: str = 'today', project: str = None) -> str:
    """
    ç”Ÿæˆæ¯æ—¥å·¥ä½œæ€»ç»“ã€‚

    èšåˆå½“æ—¥æ‰€æœ‰ä¼šè¯çš„ä»»åŠ¡æ‘˜è¦å’Œé”šç‚¹æ•°é‡ã€‚

    Args:
        date: æ—¥æœŸ
        project: å¯é€‰é¡¹ç›®è¿‡æ»¤

    Returns:
        æ ¼å¼åŒ–çš„æ¯æ—¥æ€»ç»“å­—ç¬¦ä¸²
    """
    sessions = query_sessions_by_date(date, project)

    if not sessions:
        date_display = date if date not in ('today', 'yesterday', 'week') else {
            'today': 'ä»Šå¤©',
            'yesterday': 'æ˜¨å¤©',
            'week': 'æœ¬å‘¨'
        }.get(date, date)
        return f"## {date_display}å·¥ä½œæ€»ç»“\n\næš‚æ— ä¼šè¯è®°å½•ã€‚"

    # ç»Ÿè®¡
    total_anchors = sum(s.get('anchor_count', 0) for s in sessions)
    projects_touched = set()
    task_summaries = []

    for session in sessions:
        project_path = session.get('project_path', '')
        if project_path:
            projects_touched.add(Path(project_path).name)

        summary = session.get('task_summary', '').strip()
        if summary:
            task_summaries.append(f"- {summary}")

    # æ ¼å¼åŒ–è¾“å‡º
    date_display = date if date not in ('today', 'yesterday', 'week') else {
        'today': 'ä»Šå¤©',
        'yesterday': 'æ˜¨å¤©',
        'week': 'æœ¬å‘¨'
    }.get(date, date)

    lines = [
        f"## {date_display}å·¥ä½œæ€»ç»“",
        "",
        f"**ä¼šè¯æ•°**: {len(sessions)}",
        f"**æ¶‰åŠé¡¹ç›®**: {', '.join(projects_touched) if projects_touched else 'æ— '}",
        f"**æ²‰æ·€é”šç‚¹**: {total_anchors}",
        "",
    ]

    if task_summaries:
        lines.append("### ä»»åŠ¡æ‘˜è¦")
        lines.append("")
        lines.extend(task_summaries)

    return '\n'.join(lines)


def introspect(scope: str = 'today', project: str = None) -> str:
    """
    å†…è§‚å‘½ä»¤æ ¸å¿ƒå®ç°ã€‚

    scope æ”¯æŒ:
    - 'today': ä»Šæ—¥æ‰€æœ‰å·¥ä½œ
    - 'yesterday': æ˜¨æ—¥å·¥ä½œ
    - 'week': æœ¬å‘¨å·¥ä½œ
    - 'session': å½“å‰ä¼šè¯ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰

    Returns:
        å†…è§‚æŠ¥å‘Šå­—ç¬¦ä¸²
    """
    if scope == 'session':
        # å½“å‰ä¼šè¯å†…è§‚ - è¿”å›æç¤ºä¿¡æ¯
        return "å½“å‰ä¼šè¯å†…è§‚è¯·ä½¿ç”¨ `/wukong å†…è§‚` å‘½ä»¤è§¦å‘ PreCompact Hookã€‚"

    sessions = query_sessions_by_date(scope, project)

    if not sessions:
        scope_display = {
            'today': 'ä»Šå¤©',
            'yesterday': 'æ˜¨å¤©',
            'week': 'æœ¬å‘¨'
        }.get(scope, scope)
        return f"## å†…è§‚æŠ¥å‘Š - {scope_display}\n\næš‚æ— ä¼šè¯è®°å½•å¯ä¾›å†…è§‚ã€‚"

    # ç”Ÿæˆå†…è§‚æŠ¥å‘Š
    scope_display = {
        'today': 'ä»Šå¤©',
        'yesterday': 'æ˜¨å¤©',
        'week': 'æœ¬å‘¨'
    }.get(scope, scope)

    lines = [
        f"## å†…è§‚æŠ¥å‘Š - {scope_display}",
        "",
    ]

    # 1. å·¥ä½œæ¦‚è§ˆ
    total_anchors = sum(s.get('anchor_count', 0) for s in sessions)
    projects_touched = set()
    for s in sessions:
        p = s.get('project_path', '')
        if p:
            projects_touched.add(Path(p).name)

    lines.extend([
        "### å·¥ä½œæ¦‚è§ˆ",
        "",
        f"- **ä¼šè¯æ•°**: {len(sessions)}",
        f"- **æ¶‰åŠé¡¹ç›®**: {', '.join(sorted(projects_touched)) if projects_touched else 'æ— '}",
        f"- **æ²‰æ·€é”šç‚¹**: {total_anchors}",
        "",
    ])

    # 2. ä¼šè¯è¯¦æƒ…
    lines.extend([
        "### ä¼šè¯è¯¦æƒ…",
        "",
    ])

    for i, session in enumerate(sessions, 1):
        session_id = session.get('session_id', 'unknown')[:8]
        project_name = Path(session.get('project_path', '')).name or 'unknown'
        task_summary = session.get('task_summary', '(æ— æ‘˜è¦)')
        anchor_count = session.get('anchor_count', 0)
        updated_at = session.get('updated_at', '')[:16].replace('T', ' ')

        lines.extend([
            f"**{i}. [{project_name}] {session_id}**",
            f"   - æ—¶é—´: {updated_at}",
            f"   - ä»»åŠ¡: {task_summary}",
            f"   - é”šç‚¹: {anchor_count}",
            "",
        ])

    # 3. åæ€æç¤º
    lines.extend([
        "### æœ«é‚£è¯†æ‰«æ",
        "",
        "> ä»¥ä¸‹é—®é¢˜å¸®åŠ©è¯†åˆ«æ½œåœ¨çš„è®¤çŸ¥åå·®:",
        "",
        "- æ˜¯å¦æœ‰é‡å¤å‡ºç°çš„é—®é¢˜/é”™è¯¯ï¼Ÿ",
        "- æ˜¯å¦æœ‰è¢«è·³è¿‡çš„éªŒè¯æ­¥éª¤ï¼Ÿ",
        "- æ˜¯å¦æœ‰å€¼å¾—æ²‰æ·€ä½†æœªè®°å½•çš„å†³ç­–ï¼Ÿ",
        "",
    ])

    return '\n'.join(lines)


def save_context(cwd: str, compact_context: str, session_id: str):
    """
    ä¿å­˜ä¸Šä¸‹æ–‡åˆ°ç”¨æˆ·çº§åˆ«ç›®å½•ã€‚

    ç›®å½•ç»“æ„:
    ~/.wukong/context/
    â”œâ”€â”€ active/{session_id}/compact.md      # æ´»è·ƒä¼šè¯ï¼ˆæŒ‰ session éš”ç¦»ï¼‰
    â””â”€â”€ sessions/{project}-{timestamp}-{session[:8]}/  # å†å²å­˜æ¡£
    """
    project_name = get_project_name(cwd)

    # 1. ä¿å­˜åˆ°æ´»è·ƒä¼šè¯ç›®å½•ï¼ˆæŒ‰ session_id éš”ç¦»ï¼Œé¿å…å¤šä¼šè¯å†²çªï¼‰
    active_dir = get_active_session_dir(session_id)
    compact_path = active_dir / 'compact.md'
    with open(compact_path, 'w', encoding='utf-8') as f:
        f.write(compact_context)

    # ä¿å­˜å…ƒæ•°æ®ï¼ˆç”¨äºåç»­æ¢å¤æ—¶è¯†åˆ«é¡¹ç›®ï¼‰
    metadata_path = active_dir / 'metadata.json'
    metadata = {
        'session_id': session_id,
        'project': project_name,
        'cwd': cwd,
        'timestamp': datetime.now().isoformat(),
    }
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    # 2. åŒæ—¶ä¿å­˜åˆ°å†å²å­˜æ¡£ï¼ˆå¸¦é¡¹ç›®åå’Œæ—¶é—´æˆ³ï¼‰
    sessions_dir = get_sessions_archive_dir()
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    session_dir = sessions_dir / f'{project_name}-{timestamp}-{session_id[:8]}'
    session_dir.mkdir(parents=True, exist_ok=True)

    backup_path = session_dir / 'compact.md'
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(compact_context)


def output_to_claude(compact_context: str, candidates: list[dict]):
    """è¾“å‡ºç»™ Claude (ä¼šè¢«æ³¨å…¥åˆ°å‹ç¼©åçš„ä¸Šä¸‹æ–‡)"""
    print("## [æ…§] PreCompact æå–å®Œæˆ")
    print()
    print("å·²ä¿å­˜å…³é”®ä¸Šä¸‹æ–‡åˆ° `~/.wukong/context/active/{session}/compact.md`")
    print()
    if candidates:
        print(f"è¯†åˆ«åˆ° {len(candidates)} ä¸ªå€™é€‰é”šç‚¹ï¼Œå¾…åç»­é—¨æ§›æ£€æŸ¥ã€‚")
    print()
    print("å¦‚éœ€æ¢å¤è¯¦ç»†ä¿¡æ¯ï¼Œè¯»å– `~/.wukong/context/sessions/` ä¸‹å¯¹åº”æ–‡ä»¶ã€‚")


def main():
    """
    ä¸»å…¥å£å‡½æ•°ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼:
    1. PreCompact Hook æ¨¡å¼: ä» stdin è¯»å– hook è¾“å…¥ï¼Œæå–ä¸Šä¸‹æ–‡
    2. åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼: é€šè¿‡ç¯å¢ƒå˜é‡ WUKONG_COMPRESS_AVATAR=1 è§¦å‘

    ç¯å¢ƒå˜é‡:
    - WUKONG_COMPRESS_AVATAR: è®¾ä¸º 1 å¯ç”¨åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼
    - WUKONG_AVATAR_TYPE: åˆ†èº«ç±»å‹ (çœ¼/è€³/é¼»/èˆŒ/èº«/æ„ æˆ–è‹±æ–‡åˆ«å)
    - WUKONG_AVATAR_OUTPUT: è¦å‹ç¼©çš„è¾“å‡ºå†…å®¹ (æˆ–ä» stdin è¯»å–)
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼
    if os.environ.get('WUKONG_COMPRESS_AVATAR') == '1':
        _run_avatar_compress_mode()
        return

    # åŸæœ‰çš„ PreCompact Hook æ¨¡å¼
    _run_precompact_mode()


def _run_avatar_compress_mode():
    """åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼"""
    avatar_type = os.environ.get('WUKONG_AVATAR_TYPE', 'default')

    # ä»ç¯å¢ƒå˜é‡æˆ– stdin è¯»å–è¾“å‡ºå†…å®¹
    avatar_output = os.environ.get('WUKONG_AVATAR_OUTPUT', '')
    if not avatar_output:
        avatar_output = sys.stdin.read()

    if not avatar_output:
        print("## [æ…§] æ— åˆ†èº«è¾“å‡ºå¯å‹ç¼©", file=sys.stderr)
        return

    # å‹ç¼©è¾“å‡º
    compressed = compress_avatar_output(avatar_output, avatar_type)

    # è¾“å‡ºå‹ç¼©ç»“æœ
    print(compressed)

    # è®°å½•æ—¥å¿—
    log_path = Path.home() / '.wukong' / 'hooks' / 'hui-compress.log'
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"\n--- {datetime.now().isoformat()} ---\n")
        log.write(f"Avatar: {avatar_type}\n")
        log.write(f"Original: {len(avatar_output)} chars\n")
        log.write(f"Compressed: {len(compressed)} chars\n")
        log.write(f"Ratio: {len(compressed)/len(avatar_output)*100:.1f}%\n")


def _run_precompact_mode():
    """PreCompact Hook æ¨¡å¼"""
    # 1. è¯»å– hook è¾“å…¥
    hook_input = read_hook_input()

    # Debug: è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_path = Path.home() / '.wukong' / 'hooks' / 'hui-extract.log'
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"\n--- {datetime.now().isoformat()} ---\n")
        log.write(f"Input keys: {list(hook_input.keys())}\n")
        log.write(f"Full input: {json.dumps(hook_input, ensure_ascii=False)[:500]}\n")

    print(f"## [æ…§] Hook è§¦å‘ - è¾“å…¥: {list(hook_input.keys())}", file=sys.stderr)

    transcript_path = hook_input.get('transcript_path', '')
    session_id = hook_input.get('session_id', 'unknown')
    cwd = hook_input.get('cwd', '.')
    trigger = hook_input.get('trigger', 'unknown')

    print(f"## [æ…§] trigger={trigger}, session={session_id[:8] if session_id else 'none'}", file=sys.stderr)

    # 2. è¯»å–å¯¹è¯è®°å½•
    messages = read_transcript(transcript_path)

    if not messages:
        print("## [æ…§] æ— å¯¹è¯è®°å½•å¯æå–")
        return

    # 2.5 æ‰§è¡Œæ¢å¤æµæ°´çº¿ (ä¸‰é˜¶æ®µä¸Šä¸‹æ–‡æ¢å¤)
    recovery_result = run_recovery_pipeline(messages, cwd)

    # è®°å½•æ¢å¤ç»“æœ
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"Recovery stage: {recovery_result['stage']}\n")
        log.write(f"Usage ratio: {recovery_result['usage'].get('usage_ratio', 0):.2%}\n")
        log.write(f"Session errors: {len(recovery_result['session_errors'])}\n")
        if recovery_result.get('dcp_stats'):
            log.write(f"DCP removed: {len(recovery_result['dcp_stats'].get('removed_tools', []))}\n")

    # ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯ï¼ˆå¦‚æœæ‰§è¡Œäº† DCPï¼‰
    messages = recovery_result.get('messages', messages)

    # 3. æå–å…³é”®ä¿¡æ¯
    task = extract_current_task(messages)
    decisions = extract_decisions(messages)
    constraints = extract_constraints(messages)
    interfaces = extract_interfaces(messages)
    problems = extract_problems(messages)
    progress = extract_progress(messages)

    # 4. ç”Ÿæˆç¼©å½¢æ€ä¸Šä¸‹æ–‡
    compact_context = generate_compact_context(
        task=task,
        decisions=decisions,
        constraints=constraints,
        interfaces=interfaces,
        problems=problems,
        progress=progress
    )

    # 5. ç”Ÿæˆå€™é€‰é”šç‚¹
    candidates = generate_anchor_candidates(decisions, constraints, problems)

    # 6. ä¿å­˜åˆ°æ–‡ä»¶
    save_context(cwd, compact_context, session_id)

    # 7. ç”Ÿæˆæ…§æ¨¡å—æ ‡å‡†è¾“å‡º (æ…§â†’è¯†äº¤æ¥åè®®)
    hui_output = generate_hui_output(
        session_id=session_id,
        project_path=cwd,
        task=task,
        decisions=decisions,
        constraints=constraints,
        interfaces=interfaces,
        problems=problems,
        progress=progress,
        compact_context=compact_context,
        candidates=candidates
    )

    # 8. ä¿å­˜æ…§è¾“å‡º JSON (ä¾›è°ƒè¯•å’Œè¯†æ¨¡å—ä½¿ç”¨ï¼Œç”¨æˆ·çº§åˆ«)
    project_name = get_project_name(cwd)
    sessions_dir = get_sessions_archive_dir()
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    session_dir = sessions_dir / f'{project_name}-{timestamp}-{session_id[:8]}'
    session_dir.mkdir(parents=True, exist_ok=True)

    hui_output_path = session_dir / 'hui-output.json'
    with open(hui_output_path, 'w', encoding='utf-8') as f:
        json.dump(hui_output, f, ensure_ascii=False, indent=2)

    # 9. è°ƒç”¨è¯†æ¨¡å—å†™å…¥
    shi_result = shi_write(hui_output, cwd)

    # è®°å½•è¯†æ¨¡å—ç»“æœ
    shi_result_path = session_dir / 'shi-result.json'
    with open(shi_result_path, 'w', encoding='utf-8') as f:
        json.dump(shi_result, f, ensure_ascii=False, indent=2)

    # è®°å½•æ—¥å¿—
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"Shi result: written={len(shi_result['anchors_written'])}, ")
        log.write(f"skipped={len(shi_result['anchors_skipped'])}, ")
        log.write(f"duplicated={len(shi_result['anchors_duplicated'])}\n")

    # 10. è¾“å‡ºç»™ Claude (åŒ…å«æ¢å¤æç¤º)
    output_to_claude_with_recovery(
        compact_context, candidates, shi_result, recovery_result
    )


def output_to_claude_with_shi_result(
    compact_context: str,
    candidates: list[dict],
    shi_result: dict
):
    """è¾“å‡ºç»™ Claudeï¼ŒåŒ…å«è¯†æ¨¡å—çš„å†™å…¥ç»“æœï¼ˆæ—§ç‰ˆï¼Œä¿ç•™å…¼å®¹ï¼‰"""
    output_to_claude_with_recovery(compact_context, candidates, shi_result, {})


def output_to_claude_with_recovery(
    compact_context: str,
    candidates: list[dict],
    shi_result: dict,
    recovery_result: dict
):
    """è¾“å‡ºç»™ Claudeï¼ŒåŒ…å«è¯†æ¨¡å—å†™å…¥ç»“æœå’Œæ¢å¤æç¤º"""
    # 1. æ¢å¤æç¤º (å¦‚æœæœ‰)
    recovery_prompt = recovery_result.get('prompt', '')
    if recovery_prompt:
        print(recovery_prompt)
        print()

    # 2. ä¸Šä¸‹æ–‡ä½¿ç”¨ç»Ÿè®¡
    usage = recovery_result.get('usage', {})
    if usage:
        ratio = usage.get('usage_ratio', 0)
        stage = recovery_result.get('stage', 'normal')
        stage_emoji = {'normal': 'ğŸŸ¢', 'warning': 'ğŸŸ¡', 'preemptive': 'ğŸŸ ', 'emergency': 'ğŸ”´'}
        print(f"**ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡**: {stage_emoji.get(stage, 'âšª')} {int(ratio * 100)}%")
        print()

    # 3. æ ‡å‡†è¾“å‡º
    print("## [æ…§] PreCompact æå–å®Œæˆ")
    print()
    print("å·²ä¿å­˜å…³é”®ä¸Šä¸‹æ–‡åˆ° `~/.wukong/context/active/{session}/compact.md`")
    print()

    if candidates:
        print(f"è¯†åˆ«åˆ° {len(candidates)} ä¸ªå€™é€‰é”šç‚¹ã€‚")

    # 4. æ˜¾ç¤ºè¯†æ¨¡å—å†™å…¥ç»“æœ
    written = shi_result.get('anchors_written', [])
    skipped = shi_result.get('anchors_skipped', [])
    duplicated = shi_result.get('anchors_duplicated', [])

    if written:
        print(f"\n### [è¯†] å·²å†™å…¥ {len(written)} ä¸ªé”šç‚¹:")
        for w in written[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            print(f"  - [{w['new_id']}] {w.get('title', '')[:40]}")

    if skipped:
        print(f"\n### [è¯†] è·³è¿‡ {len(skipped)} ä¸ªé”šç‚¹ (æœªè¾¾é—¨æ§›)")

    if duplicated:
        print(f"\n### [è¯†] å»é‡ {len(duplicated)} ä¸ªé”šç‚¹ (å·²å­˜åœ¨)")

    errors = shi_result.get('errors', [])
    if errors:
        print(f"\n### [è¯†] é”™è¯¯: {len(errors)} ä¸ª")
        for e in errors[:3]:
            print(f"  - {e.get('id')}: {e.get('error')}")

    # 5. DCP ç»Ÿè®¡ (å¦‚æœæ‰§è¡Œäº†)
    dcp_stats = recovery_result.get('dcp_stats')
    if dcp_stats:
        removed = dcp_stats.get('removed_tools', [])
        if removed:
            print(f"\n### [DCP] åŠ¨æ€å‰ªæ:")
            print(f"  - ç§»é™¤äº† {len(removed)} ä¸ªå†—ä½™å·¥å…·è°ƒç”¨")
            print(f"  - æ¶ˆæ¯æ•°: {dcp_stats.get('original_count', 0)} â†’ {dcp_stats.get('pruned_count', 0)}")

    # 6. ä¼šè¯é”™è¯¯ (å¦‚æœæœ‰)
    session_errors = recovery_result.get('session_errors', [])
    if session_errors:
        print(f"\n### [æ¢å¤] æ£€æµ‹åˆ° {len(session_errors)} ä¸ªä¼šè¯é—®é¢˜ (å·²å¤„ç†)")

    print()
    print("å¦‚éœ€æ¢å¤è¯¦ç»†ä¿¡æ¯ï¼Œè¯»å– `~/.wukong/context/sessions/` ä¸‹å¯¹åº”æ–‡ä»¶ã€‚")


# ============================================================
# ä»»åŠ¡ç»­æœŸæœºåˆ¶ (Task Continuation)
# å€Ÿé‰´è‡ª oh-my-opencode çš„ Ralph Loop å’Œ Todo Continuation Enforcer
# ============================================================

# ä»»åŠ¡ç»­æœŸé…ç½®
CONTINUATION_CONFIG = {
    'max_iterations': 100,          # æœ€å¤§ç»­æœŸæ¬¡æ•°
    'completion_markers': [         # å®Œæˆæ ‡è®°
        '<promise>DONE</promise>',
        '## ä»»åŠ¡å®Œæˆ',
        '## Task Complete',
        'âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ',
    ],
    'incomplete_patterns': [        # æœªå®Œæˆæ ‡è®°
        'in_progress',
        'å¾…å®Œæˆ',
        'TODO:',
        'ç»§ç»­',
        'ä¸‹ä¸€æ­¥',
    ],
}


def detect_incomplete_tasks(messages: list[dict], cwd: str) -> dict:
    """
    æ£€æµ‹æœªå®Œæˆçš„ä»»åŠ¡ã€‚

    æ£€æŸ¥ç­–ç•¥ï¼š
    1. æ£€æŸ¥æœ€åå‡ æ¡æ¶ˆæ¯æ˜¯å¦æœ‰å®Œæˆæ ‡è®°
    2. æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æœªå®Œæˆæ ‡è®°
    3. æ£€æŸ¥ .wukong/context/current/ ä¸‹æ˜¯å¦æœ‰æœªå®Œæˆä»»åŠ¡è®°å½•

    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        cwd: å½“å‰å·¥ä½œç›®å½•

    Returns:
        {
            'has_incomplete': bool,
            'incomplete_tasks': list[str],
            'iteration': int,
            'reason': str
        }
    """
    result = {
        'has_incomplete': False,
        'incomplete_tasks': [],
        'iteration': 0,
        'reason': ''
    }

    # 1. æ£€æŸ¥å®Œæˆæ ‡è®°
    recent_content = ''
    for msg in messages[-5:]:
        recent_content += get_message_content(msg) + '\n'

    for marker in CONTINUATION_CONFIG['completion_markers']:
        if marker in recent_content:
            result['reason'] = f'Found completion marker: {marker}'
            return result

    # 2. æ£€æŸ¥æœªå®Œæˆæ ‡è®°
    incomplete_found = []
    for pattern in CONTINUATION_CONFIG['incomplete_patterns']:
        if pattern.lower() in recent_content.lower():
            incomplete_found.append(pattern)

    if incomplete_found:
        result['has_incomplete'] = True
        result['incomplete_tasks'] = incomplete_found
        result['reason'] = f'Found incomplete markers: {incomplete_found}'

    # 3. æ£€æŸ¥æœ¬åœ°ä»»åŠ¡çŠ¶æ€æ–‡ä»¶
    state_file = Path(cwd) / '.wukong' / 'context' / 'current' / 'task-state.json'
    if state_file.exists():
        try:
            with open(state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)
                result['iteration'] = state.get('iteration', 0)
                if state.get('incomplete_tasks'):
                    result['has_incomplete'] = True
                    result['incomplete_tasks'].extend(state['incomplete_tasks'])
        except (json.JSONDecodeError, IOError):
            pass

    return result


def generate_continuation_prompt(
    task: str,
    incomplete_tasks: list[str],
    iteration: int,
    max_iterations: int = 100
) -> str:
    """
    ç”Ÿæˆä»»åŠ¡ç»­æœŸæç¤ºã€‚

    å€Ÿé‰´ Ralph Loop çš„æ ¼å¼ï¼Œæ¸…æ™°å‘ŠçŸ¥ï¼š
    - å½“å‰è¿­ä»£æ¬¡æ•°
    - åŸå§‹ä»»åŠ¡
    - æœªå®Œæˆé¡¹
    - ç»§ç»­æŒ‡ä»¤

    Args:
        task: åŸå§‹ä»»åŠ¡æè¿°
        incomplete_tasks: æœªå®Œæˆä»»åŠ¡åˆ—è¡¨
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

    Returns:
        ç»­æœŸæç¤ºå­—ç¬¦ä¸²
    """
    prompt_lines = [
        "## [WUKONG CONTINUATION]",
        "",
        f"**è¿­ä»£**: [{iteration + 1}/{max_iterations}]",
        "",
        f"**åŸå§‹ä»»åŠ¡**: {task[:200]}",
        "",
    ]

    if incomplete_tasks:
        prompt_lines.append("**æœªå®Œæˆé¡¹**:")
        for t in incomplete_tasks[:5]:
            prompt_lines.append(f"- {t}")
        prompt_lines.append("")

    prompt_lines.extend([
        "**æŒ‡ä»¤**: è¯·æ£€æŸ¥å½“å‰è¿›åº¦ï¼Œå¦‚ä»»åŠ¡æœªå®Œæˆè¯·ç»§ç»­æ‰§è¡Œã€‚",
        "",
        "å¦‚æœå·²å®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼Œè¯·è¾“å‡º `## ä»»åŠ¡å®Œæˆ` æ ‡è®°ã€‚",
    ])

    return '\n'.join(prompt_lines)


def save_task_state(cwd: str, state: dict):
    """
    ä¿å­˜ä»»åŠ¡çŠ¶æ€åˆ°æ–‡ä»¶ã€‚

    ç”¨äºè·¨å‹ç¼©å‘¨æœŸè¿½è¸ªä»»åŠ¡è¿›åº¦ã€‚

    Args:
        cwd: å½“å‰å·¥ä½œç›®å½•
        state: ä»»åŠ¡çŠ¶æ€å­—å…¸
    """
    state_dir = Path(cwd) / '.wukong' / 'context' / 'current'
    state_dir.mkdir(parents=True, exist_ok=True)

    state_file = state_dir / 'task-state.json'
    state['updated_at'] = datetime.now().isoformat()

    with open(state_file, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def clear_task_state(cwd: str):
    """æ¸…é™¤ä»»åŠ¡çŠ¶æ€æ–‡ä»¶ï¼ˆä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ï¼‰"""
    state_file = Path(cwd) / '.wukong' / 'context' / 'current' / 'task-state.json'
    if state_file.exists():
        state_file.unlink()


# ============================================================
# é”™è¯¯åˆ†ç±»ä¸æ¢å¤ (Error Classification & Recovery)
# å€Ÿé‰´è‡ª oh-my-opencode çš„ç»†ç²’åº¦é”™è¯¯æ¢å¤æœºåˆ¶
# ============================================================

# é”™è¯¯åˆ†ç±»ä½“ç³»
ERROR_CATEGORIES = {
    'edit_failure': {
        'patterns': [
            'oldString not found',
            'oldString found multiple times',
            'oldString and newString must be different',
        ],
        'severity': 'HIGH',
        'recovery': 'read_and_retry',
    },
    'tool_result_missing': {
        'patterns': [
            'tool_use',
            'tool_result',
        ],
        'severity': 'HIGH',
        'recovery': 'inject_placeholder',
    },
    'context_exceeded': {
        'patterns': [
            'context_length_exceeded',
            'prompt is too long',
            'token limit',
        ],
        'severity': 'CRITICAL',
        'recovery': 'compress_and_retry',
    },
    'permission_denied': {
        'patterns': [
            'Permission denied',
            'EACCES',
            'Operation not permitted',
        ],
        'severity': 'HIGH',
        'recovery': 'user_confirm',
    },
    'file_not_found': {
        'patterns': [
            'No such file',
            'ENOENT',
            'FileNotFoundError',
        ],
        'severity': 'MEDIUM',
        'recovery': 'search_and_retry',
    },
}


def classify_error(error_message: str) -> dict | None:
    """
    å¯¹é”™è¯¯æ¶ˆæ¯è¿›è¡Œåˆ†ç±»ã€‚

    Args:
        error_message: é”™è¯¯æ¶ˆæ¯å­—ç¬¦ä¸²

    Returns:
        {
            'category': str,
            'severity': str,
            'recovery': str,
            'matched_pattern': str
        } æˆ– None
    """
    if not error_message:
        return None

    error_lower = error_message.lower()

    for category, config in ERROR_CATEGORIES.items():
        for pattern in config['patterns']:
            if pattern.lower() in error_lower:
                return {
                    'category': category,
                    'severity': config['severity'],
                    'recovery': config['recovery'],
                    'matched_pattern': pattern,
                }

    return None


def generate_recovery_prompt(error_info: dict) -> str:
    """
    æ ¹æ®é”™è¯¯åˆ†ç±»ç”Ÿæˆæ¢å¤æç¤ºã€‚

    Args:
        error_info: classify_error è¿”å›çš„é”™è¯¯ä¿¡æ¯

    Returns:
        æ¢å¤æŒ‡å¯¼å­—ç¬¦ä¸²
    """
    category = error_info.get('category', '')
    recovery = error_info.get('recovery', '')

    prompts = {
        'read_and_retry': """
## [ERROR RECOVERY - Edit Failure]

**STOP and do this NOW:**

1. **READ** the target file to see its ACTUAL current state
2. **VERIFY** what the content really looks like (your assumption was wrong)
3. **ACKNOWLEDGE** the error - understand why oldString wasn't found
4. **CORRECTED** action based on actual file state
5. **DO NOT** retry the same edit without verification

**Common causes:**
- File was modified by another operation
- Indentation/whitespace mismatch
- String was already changed
""",
        'inject_placeholder': """
## [ERROR RECOVERY - Tool Result Missing]

A tool call is missing its result. This may be due to:
- User interruption (ESC pressed)
- Network timeout
- Tool execution failure

**Recovery:** The system will inject a placeholder result. Please review and retry the operation if needed.
""",
        'compress_and_retry': """
## [ERROR RECOVERY - Context Exceeded]

The conversation has exceeded the context window limit.

**Automatic recovery in progress:**
1. Extracting key anchors and decisions
2. Compressing tool outputs
3. Generating compact summary

Please wait for compression to complete, then continue your task.
""",
        'user_confirm': """
## [ERROR RECOVERY - Permission Denied]

The operation requires elevated permissions.

**Please confirm:**
- Is the target path correct?
- Do you have write access?
- Is the file locked by another process?

If you want to proceed, please grant the necessary permissions or modify the target path.
""",
        'search_and_retry': """
## [ERROR RECOVERY - File Not Found]

The specified file does not exist.

**Recovery steps:**
1. Use Glob/Grep to search for similar files
2. Check if the file was moved or renamed
3. Verify the path is correct

**Do NOT** assume the path - verify it first.
""",
    }

    return prompts.get(recovery, f"## [ERROR] Unknown error category: {category}")


# ============================================================
# ä¸‰é˜¶æ®µä¸Šä¸‹æ–‡æ¢å¤ (Three-Stage Context Recovery)
# å€Ÿé‰´è‡ª oh-my-opencode çš„ anthropic-context-window-limit-recovery
# ============================================================

# ä¸Šä¸‹æ–‡ä½¿ç”¨é˜ˆå€¼
CONTEXT_THRESHOLDS = {
    'warning': 0.70,        # 70% - å‘å‡ºè­¦å‘Š
    'preemptive': 0.85,     # 85% - ä¸»åŠ¨å‹ç¼©
    'emergency': 1.0,       # 100% - ç´§æ€¥æ•‘æ´
}


def estimate_context_usage(messages: list[dict]) -> dict:
    """
    ä¼°ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡ã€‚

    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªä¼°ç®—ï¼Œå®é™… token æ•°å–å†³äºå…·ä½“çš„ tokenizerã€‚
    ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ (è‹±æ–‡) æˆ– 1.5 å­—ç¬¦ (ä¸­æ–‡)

    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

    Returns:
        {
            'total_chars': int,
            'estimated_tokens': int,
            'usage_ratio': float,  # 0.0 - 1.0
            'stage': str  # 'normal', 'warning', 'preemptive', 'emergency'
        }
    """
    # Claude çš„ä¸Šä¸‹æ–‡çª—å£å¤§å° (200k tokens for Claude 3)
    MAX_TOKENS = 200000

    total_chars = 0
    for msg in messages:
        content = get_message_content(msg)
        total_chars += len(content)

        # å·¥å…·è°ƒç”¨çš„è¾“å‡ºé€šå¸¸å¾ˆå¤§
        if 'toolUseResult' in msg:
            result = msg.get('toolUseResult', '')
            if isinstance(result, str):
                total_chars += len(result)
            elif isinstance(result, dict):
                total_chars += len(json.dumps(result, ensure_ascii=False))

    # ç²—ç•¥ä¼°ç®— token æ•° (æ··åˆè¯­è¨€ï¼Œå–å¹³å‡)
    estimated_tokens = total_chars // 3

    usage_ratio = estimated_tokens / MAX_TOKENS

    # ç¡®å®šé˜¶æ®µ
    if usage_ratio >= CONTEXT_THRESHOLDS['emergency']:
        stage = 'emergency'
    elif usage_ratio >= CONTEXT_THRESHOLDS['preemptive']:
        stage = 'preemptive'
    elif usage_ratio >= CONTEXT_THRESHOLDS['warning']:
        stage = 'warning'
    else:
        stage = 'normal'

    return {
        'total_chars': total_chars,
        'estimated_tokens': estimated_tokens,
        'usage_ratio': usage_ratio,
        'stage': stage,
    }


def generate_stage_prompt(stage: str, usage_ratio: float) -> str:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡é˜¶æ®µç”Ÿæˆæç¤ºã€‚

    Args:
        stage: é˜¶æ®µ ('normal', 'warning', 'preemptive', 'emergency')
        usage_ratio: ä½¿ç”¨ç‡ (0.0 - 1.0)

    Returns:
        é˜¶æ®µæç¤ºå­—ç¬¦ä¸²
    """
    percentage = int(usage_ratio * 100)

    if stage == 'warning':
        return f"""
## [CONTEXT MONITOR] âš ï¸ 70% è­¦å‘Š

**å½“å‰ä½¿ç”¨ç‡**: {percentage}%

**æé†’**: ä¸Šä¸‹æ–‡çª—å£å·²ä½¿ç”¨ {percentage}%ï¼Œä½†ä»æœ‰å……è¶³ç©ºé—´ã€‚
- âœ… ä¸è¦å› æ­¤ä»“ä¿ƒè¡ŒåŠ¨
- âœ… ç»§ç»­é«˜è´¨é‡å®Œæˆå½“å‰ä»»åŠ¡
- âš ï¸ è€ƒè™‘åœ¨åˆé€‚æ—¶æœºæ‰§è¡Œ `/wukong å‹ç¼©`
"""

    elif stage == 'preemptive':
        return f"""
## [CONTEXT MONITOR] ğŸŸ  85% ä¸»åŠ¨å‹ç¼©

**å½“å‰ä½¿ç”¨ç‡**: {percentage}%

**è‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œ**:
1. âœ… å¯åŠ¨ DCP (åŠ¨æ€ä¸Šä¸‹æ–‡å‰ªæ)
2. âœ… å‹ç¼©å¤§å‹å·¥å…·è¾“å‡º
3. âœ… ä¿ç•™å…³é”®é”šç‚¹å’Œå†³ç­–

**ä½ åº”è¯¥**:
- å®Œæˆå½“å‰æ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡
- ç„¶åæ‰§è¡Œ `/wukong å‹ç¼©` ä¿å­˜è¿›åº¦
"""

    elif stage == 'emergency':
        return f"""
## [CONTEXT MONITOR] ğŸ”´ 100% ç´§æ€¥æ•‘æ´

**å½“å‰ä½¿ç”¨ç‡**: {percentage}%

**ç´§æ€¥æ¢å¤æ¨¡å¼å·²æ¿€æ´»**:
1. ğŸš¨ æ‰§è¡Œå®Œæ•´ä¸Šä¸‹æ–‡æ‘˜è¦
2. ğŸš¨ ä¿ç•™ AGENTS.md å’Œå…³é”®ä¸Šä¸‹æ–‡
3. ğŸš¨ å‡†å¤‡ç»§ç»­æ‰§è¡ŒæŒ‡ä»¤

**è‡ªåŠ¨ä¿å­˜çš„å†…å®¹**:
- å½“å‰ä»»åŠ¡æè¿°
- å…³é”®å†³ç­–å’Œçº¦æŸ
- æœªå®Œæˆä»»åŠ¡åˆ—è¡¨

ç»§ç»­ä½ çš„ä»»åŠ¡ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ¢å¤ä¸Šä¸‹æ–‡ã€‚
"""

    return ""


# ============================================================
# DCP - åŠ¨æ€ä¸Šä¸‹æ–‡å‰ªæ (Dynamic Context Pruning)
# ============================================================

# å—ä¿æŠ¤çš„å·¥å…·åˆ—è¡¨ (ä¸ä¼šè¢«å‰ªæ)
PROTECTED_TOOLS = {
    'Task', 'TodoWrite', 'lsp_rename', 'Edit', 'Write',
}

# å¯å®‰å…¨å‰ªæçš„å·¥å…·æ¨¡å¼
PRUNABLE_PATTERNS = {
    'Glob': {'keep_last': 3},       # åªä¿ç•™æœ€è¿‘ 3 æ¬¡
    'Grep': {'keep_last': 3},
    'Read': {'keep_last': 5},       # è¯»å–æ“ä½œä¿ç•™æ›´å¤š
    'Bash': {'keep_last': 3},
    'WebSearch': {'keep_last': 2},
    'WebFetch': {'keep_last': 2},
}


def apply_dcp(messages: list[dict]) -> tuple[list[dict], dict]:
    """
    åº”ç”¨åŠ¨æ€ä¸Šä¸‹æ–‡å‰ªæ (DCP)ã€‚

    ç­–ç•¥:
    1. è¯†åˆ«é‡å¤çš„å·¥å…·è°ƒç”¨ (ç›¸åŒç­¾å)
    2. ä¿æŠ¤å…³é”®å·¥å…·çš„è¾“å‡º
    3. ç§»é™¤å†—ä½™ï¼Œåªä¿ç•™æœ€æ–°

    Args:
        messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨

    Returns:
        (pruned_messages, stats): å‰ªæåçš„æ¶ˆæ¯åˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    """
    stats = {
        'original_count': len(messages),
        'pruned_count': 0,
        'removed_tools': [],
    }

    # æŒ‰å·¥å…·ç±»å‹åˆ†ç»„
    tool_calls = {}  # tool_name -> list of (index, message)

    for i, msg in enumerate(messages):
        # æ£€æµ‹å·¥å…·è°ƒç”¨
        if msg.get('type') == 'assistant':
            content = msg.get('message', {}).get('content', [])
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'tool_use':
                        tool_name = item.get('name', '')
                        if tool_name and tool_name not in PROTECTED_TOOLS:
                            if tool_name not in tool_calls:
                                tool_calls[tool_name] = []
                            tool_calls[tool_name].append((i, msg))

    # ç¡®å®šè¦ç§»é™¤çš„æ¶ˆæ¯ç´¢å¼•
    indices_to_remove = set()

    for tool_name, calls in tool_calls.items():
        config = PRUNABLE_PATTERNS.get(tool_name, {'keep_last': 3})
        keep_last = config.get('keep_last', 3)

        if len(calls) > keep_last:
            # ç§»é™¤æ—§çš„è°ƒç”¨
            for idx, _ in calls[:-keep_last]:
                indices_to_remove.add(idx)
                stats['removed_tools'].append(tool_name)

    # æ„å»ºå‰ªæåçš„æ¶ˆæ¯åˆ—è¡¨
    pruned_messages = [
        msg for i, msg in enumerate(messages)
        if i not in indices_to_remove
    ]

    stats['pruned_count'] = len(pruned_messages)

    return pruned_messages, stats


def truncate_large_outputs(messages: list[dict], target_reduction: float = 0.5) -> list[dict]:
    """
    æˆªæ–­å¤§å‹å·¥å…·è¾“å‡ºã€‚

    ç­–ç•¥:
    - æŒ‰å¤§å°æ’åºå·¥å…·è¾“å‡º
    - æˆªæ–­æœ€å¤§çš„è¾“å‡º (ç›®æ ‡å‰Šå‡ 50%)
    - ä¿ç•™å…ƒæ•°æ® (å·¥å…·åã€çŠ¶æ€)

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        target_reduction: ç›®æ ‡å‰Šå‡æ¯”ä¾‹

    Returns:
        å¤„ç†åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    MAX_TOOL_OUTPUT = 2000  # å•ä¸ªå·¥å…·è¾“å‡ºçš„æœ€å¤§å­—ç¬¦æ•°

    processed = []
    for msg in messages:
        if 'toolUseResult' in msg:
            result = msg.get('toolUseResult', '')
            if isinstance(result, str) and len(result) > MAX_TOOL_OUTPUT:
                # æˆªæ–­å¹¶æ·»åŠ æ ‡è®°
                msg = msg.copy()
                msg['toolUseResult'] = result[:MAX_TOOL_OUTPUT] + '\n... [OUTPUT TRUNCATED]'
            elif isinstance(result, dict):
                result_str = json.dumps(result, ensure_ascii=False)
                if len(result_str) > MAX_TOOL_OUTPUT:
                    msg = msg.copy()
                    # ä¿ç•™å…³é”®å­—æ®µ
                    truncated = {
                        '_truncated': True,
                        '_original_size': len(result_str),
                    }
                    for key in ['status', 'error', 'summary', 'count']:
                        if key in result:
                            truncated[key] = result[key]
                    msg['toolUseResult'] = truncated

        processed.append(msg)

    return processed


# ============================================================
# ä¼šè¯çº§é”™è¯¯æ¢å¤ (Session-Level Error Recovery)
# ============================================================

def detect_session_errors(messages: list[dict]) -> list[dict]:
    """
    æ£€æµ‹ä¼šè¯çº§é”™è¯¯ã€‚

    æ£€æµ‹ç±»å‹:
    1. ç¼ºå¤±å·¥å…·ç»“æœ (tool_use æ— å¯¹åº” tool_result)
    2. ç©ºæ¶ˆæ¯
    3. æ€è€ƒå—é—®é¢˜

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        é”™è¯¯åˆ—è¡¨ [{'type': str, 'index': int, 'details': str}]
    """
    errors = []

    pending_tool_uses = {}  # tool_use_id -> index

    for i, msg in enumerate(messages):
        # æ£€æµ‹ç©ºæ¶ˆæ¯
        content = get_message_content(msg)
        if msg.get('type') == 'assistant' and not content.strip():
            errors.append({
                'type': 'empty_message',
                'index': i,
                'details': 'Assistant message has no content'
            })

        # è·Ÿè¸ªå·¥å…·è°ƒç”¨
        if msg.get('type') == 'assistant':
            msg_content = msg.get('message', {}).get('content', [])
            if isinstance(msg_content, list):
                for item in msg_content:
                    if isinstance(item, dict) and item.get('type') == 'tool_use':
                        tool_id = item.get('id', '')
                        if tool_id:
                            pending_tool_uses[tool_id] = i

        # æ£€æµ‹å·¥å…·ç»“æœ
        if msg.get('type') == 'user':
            msg_content = msg.get('message', {}).get('content', [])
            if isinstance(msg_content, list):
                for item in msg_content:
                    if isinstance(item, dict) and item.get('type') == 'tool_result':
                        tool_id = item.get('tool_use_id', '')
                        if tool_id in pending_tool_uses:
                            del pending_tool_uses[tool_id]

    # æœªé…å¯¹çš„å·¥å…·è°ƒç”¨
    for tool_id, index in pending_tool_uses.items():
        errors.append({
            'type': 'tool_result_missing',
            'index': index,
            'details': f'Tool use {tool_id} has no corresponding result'
        })

    return errors


def generate_recovery_for_session_errors(errors: list[dict]) -> str:
    """
    ä¸ºä¼šè¯é”™è¯¯ç”Ÿæˆæ¢å¤æç¤ºã€‚

    Args:
        errors: é”™è¯¯åˆ—è¡¨

    Returns:
        æ¢å¤æç¤ºå­—ç¬¦ä¸²
    """
    if not errors:
        return ""

    lines = [
        "## [SESSION RECOVERY] æ£€æµ‹åˆ°ä¼šè¯é”™è¯¯",
        "",
    ]

    error_types = {}
    for e in errors:
        et = e['type']
        if et not in error_types:
            error_types[et] = []
        error_types[et].append(e)

    if 'tool_result_missing' in error_types:
        count = len(error_types['tool_result_missing'])
        lines.append(f"- **ç¼ºå¤±å·¥å…·ç»“æœ**: {count} ä¸ª")
        lines.append("  â†’ ç³»ç»Ÿå·²æ³¨å…¥å ä½ç¬¦ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ“ä½œæ˜¯å¦éœ€è¦é‡è¯•")

    if 'empty_message' in error_types:
        count = len(error_types['empty_message'])
        lines.append(f"- **ç©ºæ¶ˆæ¯**: {count} ä¸ª")
        lines.append("  â†’ å·²è‡ªåŠ¨æ¸…ç†")

    lines.extend([
        "",
        "**å»ºè®®**: æ£€æŸ¥æœ€è¿‘çš„æ“ä½œæ˜¯å¦æˆåŠŸå®Œæˆï¼Œå¦‚æœ‰éœ€è¦è¯·é‡è¯•ã€‚",
    ])

    return '\n'.join(lines)


# ============================================================
# ä¸»æ¢å¤æµç¨‹ (Main Recovery Flow)
# ============================================================

def run_recovery_pipeline(
    messages: list[dict],
    cwd: str
) -> dict:
    """
    æ‰§è¡Œå®Œæ•´çš„æ¢å¤æµæ°´çº¿ã€‚

    æµç¨‹:
    1. ä¼°ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡
    2. æ£€æµ‹ä¼šè¯é”™è¯¯
    3. æ ¹æ®é˜¶æ®µæ‰§è¡Œå¯¹åº”æ¢å¤ç­–ç•¥
    4. ç”Ÿæˆæ¢å¤æç¤º

    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        cwd: å½“å‰å·¥ä½œç›®å½•

    Returns:
        {
            'stage': str,
            'usage': dict,
            'session_errors': list,
            'dcp_stats': dict | None,
            'prompt': str,
            'messages': list  # å¤„ç†åçš„æ¶ˆæ¯
        }
    """
    result = {
        'stage': 'normal',
        'usage': {},
        'session_errors': [],
        'dcp_stats': None,
        'prompt': '',
        'messages': messages,
    }

    # 1. ä¼°ç®—ä½¿ç”¨ç‡
    usage = estimate_context_usage(messages)
    result['usage'] = usage
    result['stage'] = usage['stage']

    # 2. æ£€æµ‹ä¼šè¯é”™è¯¯
    session_errors = detect_session_errors(messages)
    result['session_errors'] = session_errors

    prompts = []

    # 3. å¤„ç†ä¼šè¯é”™è¯¯
    if session_errors:
        error_prompt = generate_recovery_for_session_errors(session_errors)
        if error_prompt:
            prompts.append(error_prompt)

    # 4. æ ¹æ®é˜¶æ®µæ‰§è¡Œæ¢å¤
    stage = usage['stage']

    if stage == 'warning':
        # åªå‘å‡ºè­¦å‘Š
        stage_prompt = generate_stage_prompt(stage, usage['usage_ratio'])
        prompts.append(stage_prompt)

    elif stage == 'preemptive':
        # 85% - æ‰§è¡Œ DCP + æˆªæ–­
        pruned, dcp_stats = apply_dcp(messages)
        result['dcp_stats'] = dcp_stats
        result['messages'] = truncate_large_outputs(pruned)

        stage_prompt = generate_stage_prompt(stage, usage['usage_ratio'])
        prompts.append(stage_prompt)

        # æ·»åŠ  DCP ç»Ÿè®¡
        if dcp_stats['removed_tools']:
            prompts.append(f"\n**DCP ç»“æœ**: ç§»é™¤äº† {len(dcp_stats['removed_tools'])} ä¸ªå†—ä½™å·¥å…·è°ƒç”¨")

    elif stage == 'emergency':
        # 100% - ç´§æ€¥æ•‘æ´
        # æ‰§è¡Œæ›´æ¿€è¿›çš„å‰ªæ
        pruned, dcp_stats = apply_dcp(messages)
        result['dcp_stats'] = dcp_stats
        result['messages'] = truncate_large_outputs(pruned, target_reduction=0.7)

        stage_prompt = generate_stage_prompt(stage, usage['usage_ratio'])
        prompts.append(stage_prompt)

    # åˆå¹¶æç¤º
    result['prompt'] = '\n\n'.join(prompts)

    # ä¿å­˜æ¢å¤çŠ¶æ€
    save_recovery_state(cwd, result)

    return result


def save_recovery_state(cwd: str, recovery_result: dict):
    """
    ä¿å­˜æ¢å¤çŠ¶æ€åˆ°æ–‡ä»¶ã€‚

    ç”¨äºè°ƒè¯•å’Œè·¨å‹ç¼©å‘¨æœŸè¿½è¸ªã€‚

    Args:
        cwd: å½“å‰å·¥ä½œç›®å½•
        recovery_result: æ¢å¤ç»“æœå­—å…¸
    """
    state_dir = Path(cwd) / '.wukong' / 'context' / 'current'
    state_dir.mkdir(parents=True, exist_ok=True)

    state_file = state_dir / 'recovery-state.json'

    # åªä¿å­˜å…³é”®ä¿¡æ¯ï¼ˆä¸ä¿å­˜å®Œæ•´æ¶ˆæ¯ï¼‰
    state = {
        'timestamp': datetime.now().isoformat(),
        'stage': recovery_result['stage'],
        'usage': recovery_result['usage'],
        'session_errors_count': len(recovery_result['session_errors']),
        'dcp_stats': recovery_result.get('dcp_stats'),
    }

    with open(state_file, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


if __name__ == '__main__':
    main()
