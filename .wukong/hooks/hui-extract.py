#!/usr/bin/env python3
"""
æ…§ (Hui) - PreCompact Hook è„šæœ¬
åœ¨ Claude Code è‡ªåŠ¨å‹ç¼©ä¸Šä¸‹æ–‡å‰è§¦å‘ï¼Œæå–å¹¶ä¿å­˜å…³é”®ä¿¡æ¯ã€‚

ä½¿ç”¨æ–¹æ³•:
1. å°†æ­¤è„šæœ¬æ”¾åˆ° ~/.wukong/hooks/hui-extract.py
2. åœ¨ .claude/settings.json ä¸­é…ç½®:
   {
     "hooks": {
       "PreCompact": [{
         "matcher": "auto",
         "hooks": [{
           "type": "command",
           "command": "python3 ~/.wukong/hooks/hui-extract.py"
         }]
       }]
     }
   }
"""

import json
import os
import sys
import re
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Any


# ============================================================
# åˆ†èº«è¾“å‡ºå‹ç¼© (Avatar Output Compression)
# ============================================================

# åˆ†èº«ç±»å‹å¯¹åº”çš„å‹ç¼©é…ç½®
AVATAR_COMPRESS_CONFIG = {
    'çœ¼': {'max_files': 20, 'max_summary': 500},
    'explorer': {'max_files': 20, 'max_summary': 500},
    'é¼»': {'max_issues': 10, 'max_chars': 800},
    'reviewer': {'max_issues': 10, 'max_chars': 800},
    'æ–—æˆ˜èƒœä½›': {'max_chars': 2000, 'keep_diff_only': True},
    'impl': {'max_chars': 2000, 'keep_diff_only': True},
    'èº«': {'max_chars': 2000, 'keep_diff_only': True},
    'default': {'max_chars': 1000},
}


def compress_avatar_output(output: str, avatar_type: str) -> str:
    """
    æ ¹æ®åˆ†èº«ç±»å‹å‹ç¼©è¾“å‡ºå†…å®¹ã€‚

    å‹ç¼©ç­–ç•¥:
    - çœ¼åˆ†èº«: æœ€å¤š 20 ä¸ªæ–‡ä»¶ + 500 å­—æ‘˜è¦
    - é¼»åˆ†èº«: æœ€å¤š 10 ä¸ª issues
    - æ–—æˆ˜èƒœä½›: æœ€å¤š 2000 å­—ï¼Œåªä¿ç•™ diff æ‘˜è¦
    - å…¶ä»–åˆ†èº«: æœ€å¤š 1000 å­—

    Args:
        output: åˆ†èº«çš„åŸå§‹è¾“å‡º
        avatar_type: åˆ†èº«ç±»å‹ (çœ¼/è€³/é¼»/èˆŒ/èº«/æ„ æˆ–è‹±æ–‡åˆ«å)

    Returns:
        å‹ç¼©åçš„è¾“å‡º
    """
    config = AVATAR_COMPRESS_CONFIG.get(avatar_type, AVATAR_COMPRESS_CONFIG['default'])

    if avatar_type in ('çœ¼', 'explorer'):
        return _compress_explorer_output(output, config)
    elif avatar_type in ('é¼»', 'reviewer'):
        return _compress_reviewer_output(output, config)
    elif avatar_type in ('æ–—æˆ˜èƒœä½›', 'impl', 'èº«'):
        return _compress_impl_output(output, config)
    else:
        return _compress_generic_output(output, config)


def _compress_explorer_output(output: str, config: dict) -> str:
    """å‹ç¼©çœ¼åˆ†èº«è¾“å‡º: ä¿ç•™æ–‡ä»¶åˆ—è¡¨ + å‘ç°æ‘˜è¦"""
    max_files = config.get('max_files', 20)
    max_summary = config.get('max_summary', 500)

    lines = output.split('\n')
    compressed_lines = []

    # æå–æ–‡ä»¶è·¯å¾„
    file_paths = []
    file_pattern = re.compile(r'^[\s\-\*]*(/[^\s]+|\.{1,2}/[^\s]+|\w+/[^\s]+)')

    for line in lines:
        match = file_pattern.match(line.strip())
        if match:
            file_paths.append(match.group(1))

    # é™åˆ¶æ–‡ä»¶æ•°é‡
    if file_paths:
        compressed_lines.append("### å‘ç°çš„æ–‡ä»¶")
        for fp in file_paths[:max_files]:
            compressed_lines.append(f"- {fp}")
        if len(file_paths) > max_files:
            compressed_lines.append(f"- ... è¿˜æœ‰ {len(file_paths) - max_files} ä¸ªæ–‡ä»¶")

    # æå–ç»“è®º/å‘ç°éƒ¨åˆ†
    findings = _extract_findings(output)
    if findings:
        compressed_lines.append("")
        compressed_lines.append("### å‘ç°æ‘˜è¦")
        compressed_lines.append(findings[:max_summary])
        if len(findings) > max_summary:
            compressed_lines.append("...")

    return '\n'.join(compressed_lines) if compressed_lines else output[:max_summary]


def _compress_reviewer_output(output: str, config: dict) -> str:
    """å‹ç¼©é¼»åˆ†èº«è¾“å‡º: ä¿ç•™ issues åˆ—è¡¨"""
    max_issues = config.get('max_issues', 10)
    max_chars = config.get('max_chars', 800)

    # å°è¯•æå– issues
    issues = []
    issue_patterns = [
        r'(?:^|\n)[\s\-\*]*(?:issue|é—®é¢˜|warning|error|bug)[:\s]*(.+?)(?=\n[\s\-\*]*(?:issue|é—®é¢˜|warning|error|bug)|$)',
        r'(?:^|\n)\d+\.\s*(.+?)(?=\n\d+\.|$)',
        r'(?:^|\n)[\-\*]\s*(.+?)(?=\n[\-\*]|$)',
    ]

    for pattern in issue_patterns:
        matches = re.findall(pattern, output, re.IGNORECASE | re.DOTALL)
        if matches:
            issues.extend(matches)
            break

    if issues:
        compressed_lines = ["### å®¡æŸ¥é—®é¢˜"]
        for i, issue in enumerate(issues[:max_issues]):
            issue_text = issue.strip()[:150]
            compressed_lines.append(f"{i+1}. {issue_text}")
        if len(issues) > max_issues:
            compressed_lines.append(f"... è¿˜æœ‰ {len(issues) - max_issues} ä¸ªé—®é¢˜")
        return '\n'.join(compressed_lines)

    # å¦‚æœæ— æ³•æå– issuesï¼Œç›´æ¥æˆªæ–­
    return _compress_generic_output(output, config)


def _compress_impl_output(output: str, config: dict) -> str:
    """å‹ç¼©æ–—æˆ˜èƒœä½›è¾“å‡º: ä¿ç•™ diff æ‘˜è¦ï¼Œç§»é™¤å®Œæ•´ä»£ç """
    max_chars = config.get('max_chars', 2000)

    compressed_parts = []

    # 1. æå–ä¿®æ”¹æ‘˜è¦
    summary = _extract_change_summary(output)
    if summary:
        compressed_parts.append("### ä¿®æ”¹æ‘˜è¦")
        compressed_parts.append(summary)

    # 2. æå–æ–‡ä»¶å˜æ›´åˆ—è¡¨
    files_changed = _extract_files_changed(output)
    if files_changed:
        compressed_parts.append("")
        compressed_parts.append("### å˜æ›´æ–‡ä»¶")
        for f in files_changed[:10]:
            compressed_parts.append(f"- {f}")

    # 3. æå– diff æ¦‚è¦ (ä¸ä¿ç•™å®Œæ•´ä»£ç )
    diff_summary = _extract_diff_summary(output)
    if diff_summary:
        compressed_parts.append("")
        compressed_parts.append("### Diff æ¦‚è¦")
        compressed_parts.append(diff_summary[:800])

    # 4. æå–æ„å»º/æµ‹è¯•ç»“æœ
    test_result = _extract_test_result(output)
    if test_result:
        compressed_parts.append("")
        compressed_parts.append("### éªŒè¯ç»“æœ")
        compressed_parts.append(test_result)

    result = '\n'.join(compressed_parts)
    return result[:max_chars] if result else output[:max_chars]


def _compress_generic_output(output: str, config: dict) -> str:
    """é€šç”¨å‹ç¼©: æå–ç»“è®ºï¼Œé™åˆ¶å­—æ•°"""
    max_chars = config.get('max_chars', 1000)

    # å°è¯•æå–ç»“è®ºéƒ¨åˆ†
    conclusion = _extract_conclusion(output)
    if conclusion:
        return conclusion[:max_chars]

    # ç›´æ¥æˆªæ–­
    if len(output) > max_chars:
        return output[:max_chars] + "\n... [å·²æˆªæ–­]"
    return output


def _extract_findings(text: str) -> str:
    """æå–æ¢ç´¢å‘ç°"""
    patterns = [
        r'(?:å‘ç°|findings?|ç»“è®º|conclusion)[:\s]*(.+?)(?=\n\n|\Z)',
        r'(?:æ€»ç»“|summary)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()

    return ""


def _extract_change_summary(text: str) -> str:
    """æå–ä¿®æ”¹æ‘˜è¦"""
    patterns = [
        r'(?:ä¿®æ”¹æ‘˜è¦|changes?|summary)[:\s]*(.+?)(?=\n\n|\n###|\Z)',
        r'(?:å®Œæˆ|done|finished)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()[:500]

    return ""


def _extract_files_changed(text: str) -> list[str]:
    """æå–å˜æ›´çš„æ–‡ä»¶åˆ—è¡¨"""
    files = []

    # åŒ¹é…å¸¸è§çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
    file_patterns = [
        r'(?:modified|changed|created|deleted|edited)[:\s]*([^\n]+\.(?:py|js|ts|md|json|yaml|yml|toml))',
        r'(?:files?_changed|å˜æ›´æ–‡ä»¶)[:\s\[]*([^\]]+)',
        r'(?:^|\n)[\s\-\*]+(/[^\s]+\.[a-z]+)',
    ]

    for pattern in file_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        files.extend(matches)

    # å»é‡
    return list(dict.fromkeys(files))


def _extract_diff_summary(text: str) -> str:
    """æå– diff æ¦‚è¦ï¼Œç§»é™¤å®Œæ•´ä»£ç å—"""
    # ç§»é™¤ä»£ç å—
    text_no_code = re.sub(r'```[\s\S]*?```', '[ä»£ç å—å·²çœç•¥]', text)

    # æå– +/- è¡Œçš„ç»Ÿè®¡
    plus_lines = len(re.findall(r'^\+[^+]', text, re.MULTILINE))
    minus_lines = len(re.findall(r'^-[^-]', text, re.MULTILINE))

    summary_parts = []
    if plus_lines or minus_lines:
        summary_parts.append(f"+{plus_lines} -{minus_lines} è¡Œå˜æ›´")

    # æå–å‡½æ•°/ç±»å˜æ›´
    func_changes = re.findall(r'(?:def|class|function)\s+(\w+)', text_no_code)
    if func_changes:
        summary_parts.append(f"æ¶‰åŠ: {', '.join(set(func_changes)[:5])}")

    return ' | '.join(summary_parts) if summary_parts else ""


def _extract_test_result(text: str) -> str:
    """æå–æµ‹è¯•/æ„å»ºç»“æœ"""
    patterns = [
        r'((?:tests?\s+)?(?:passed|failed|success|error)[^\n]*)',
        r'((?:build|æ„å»º)\s*(?:æˆåŠŸ|å¤±è´¥|passed|failed)[^\n]*)',
        r'(âœ“|âœ—|PASS|FAIL)[^\n]*',
    ]

    results = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        results.extend(matches[:3])

    return ' | '.join(results) if results else ""


def _extract_conclusion(text: str) -> str:
    """æå–ç»“è®ºéƒ¨åˆ†"""
    patterns = [
        r'(?:ç»“è®º|conclusion|æ€»ç»“|summary|ç»“æœ|result)[:\s]*(.+?)(?=\n\n|\Z)',
        r'(?:å®Œæˆ|done|å®Œæ¯•)[:\s]*(.+?)(?=\n\n|\Z)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()

    return ""


def compress_avatar_output_from_env(output: str) -> str:
    """
    ä»ç¯å¢ƒå˜é‡è¯»å–åˆ†èº«ç±»å‹å¹¶å‹ç¼©è¾“å‡ºã€‚

    ç¯å¢ƒå˜é‡:
    - WUKONG_COMPRESS_AVATAR: è®¾ä¸º 1 å¯ç”¨å‹ç¼©
    - WUKONG_AVATAR_TYPE: åˆ†èº«ç±»å‹
    """
    if os.environ.get('WUKONG_COMPRESS_AVATAR') != '1':
        return output

    avatar_type = os.environ.get('WUKONG_AVATAR_TYPE', 'default')
    return compress_avatar_output(output, avatar_type)


def read_hook_input() -> dict[str, Any]:
    """ä» stdin è¯»å– hook è¾“å…¥"""
    try:
        return json.load(sys.stdin)
    except json.JSONDecodeError:
        return {}


def read_transcript(transcript_path: str) -> list[dict]:
    """è¯»å–å¯¹è¯è®°å½•"""
    messages = []
    if not transcript_path:
        return messages

    path = Path(transcript_path).expanduser()
    if not path.exists() or not path.is_file():
        return messages

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    messages.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return messages


def extract_decisions(messages: list[dict]) -> list[dict]:
    """æå–å†³ç­–ä¿¡æ¯"""
    decisions = []
    decision_patterns = [
        r'\[D\d+\]',  # [D001] æ ¼å¼çš„å†³ç­–å¼•ç”¨
        r'å†³å®š|å†³ç­–|é€‰æ‹©|é‡‡ç”¨|ä½¿ç”¨',  # å†³ç­–å…³é”®è¯
        r'Decision|Decided|Choose|Use',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in decision_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                # æå–åŒ…å«å†³ç­–çš„æ®µè½
                decisions.append({
                    'type': 'decision',
                    'content': content[:500],  # é™åˆ¶é•¿åº¦
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return decisions[-5:]  # åªä¿ç•™æœ€è¿‘5ä¸ª


def extract_constraints(messages: list[dict]) -> list[dict]:
    """æå–çº¦æŸä¿¡æ¯"""
    constraints = []
    constraint_patterns = [
        r'\[C\d+\]',  # [C001] æ ¼å¼çš„çº¦æŸå¼•ç”¨
        r'å¿…é¡»|ç¦æ­¢|ä¸èƒ½|ä¸å…è®¸|çº¦æŸ|é™åˆ¶',
        r'MUST|NEVER|ALWAYS|constraint',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in constraint_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                constraints.append({
                    'type': 'constraint',
                    'content': content[:300],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return constraints[-3:]


def extract_interfaces(messages: list[dict]) -> list[dict]:
    """æå–æ¥å£å®šä¹‰"""
    interfaces = []
    interface_patterns = [
        r'\[I\d+\]',  # [I001] æ ¼å¼çš„æ¥å£å¼•ç”¨
        r'def \w+\(.*\)',  # Python å‡½æ•°å®šä¹‰
        r'class \w+',  # ç±»å®šä¹‰
        r'interface|API|endpoint',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in interface_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                interfaces.append({
                    'type': 'interface',
                    'content': content[:400],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return interfaces[-3:]


def extract_problems(messages: list[dict]) -> list[dict]:
    """æå–é—®é¢˜/é™·é˜±"""
    problems = []
    problem_patterns = [
        r'\[P\d+\]',  # [P001] æ ¼å¼çš„é—®é¢˜å¼•ç”¨
        r'é—®é¢˜|bug|é”™è¯¯|å¤±è´¥|è­¦å‘Š',
        r'error|fail|warning|issue|problem',
    ]

    for msg in messages:
        content = get_message_content(msg)
        for pattern in problem_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                problems.append({
                    'type': 'problem',
                    'content': content[:300],
                    'timestamp': msg.get('timestamp', '')
                })
                break

    return problems[-3:]


def get_message_content(msg: dict) -> str:
    """è·å–æ¶ˆæ¯å†…å®¹"""
    if 'message' in msg and 'content' in msg['message']:
        content = msg['message']['content']
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return ' '.join(
                item.get('text', '')
                for item in content
                if isinstance(item, dict) and item.get('type') == 'text'
            )
    return ''


def extract_current_task(messages: list[dict]) -> str:
    """æå–å½“å‰ä»»åŠ¡æè¿°"""
    # æŸ¥æ‰¾ç”¨æˆ·çš„åˆå§‹è¯·æ±‚
    for msg in messages[:5]:  # åªçœ‹å‰å‡ æ¡
        if msg.get('type') == 'user':
            content = get_message_content(msg)
            if content:
                return content[:200]
    return "æœªçŸ¥ä»»åŠ¡"


def extract_progress(messages: list[dict]) -> dict:
    """æå–è¿›åº¦ä¿¡æ¯"""
    # ç®€å•ç»Ÿè®¡
    total_messages = len(messages)
    user_messages = sum(1 for m in messages if m.get('type') == 'user')
    assistant_messages = sum(1 for m in messages if m.get('type') == 'assistant')

    return {
        'total_turns': total_messages // 2,
        'user_messages': user_messages,
        'assistant_messages': assistant_messages,
    }


def generate_compact_context(
    task: str,
    decisions: list[dict],
    constraints: list[dict],
    interfaces: list[dict],
    problems: list[dict],
    progress: dict
) -> str:
    """ç”Ÿæˆç¼©å½¢æ€ä¸Šä¸‹æ–‡"""
    lines = [
        "## ğŸ”¸ ç¼©å½¢æ€ä¸Šä¸‹æ–‡",
        "",
        f"ã€ä»»åŠ¡ã€‘{task}",
        "",
        "ã€å·²å†³ç­–ã€‘",
    ]

    if decisions:
        for d in decisions[:3]:
            content = d['content'][:100].replace('\n', ' ')
            lines.append(f"- {content}...")
    else:
        lines.append("- (æš‚æ— )")

    lines.extend([
        "",
        "ã€çº¦æŸã€‘",
    ])

    if constraints:
        for c in constraints[:2]:
            content = c['content'][:80].replace('\n', ' ')
            lines.append(f"- {content}...")
    else:
        lines.append("- (æš‚æ— )")

    lines.extend([
        "",
        "ã€å½“å‰è¿›åº¦ã€‘",
        f"- å¯¹è¯è½®æ¬¡: {progress.get('total_turns', 0)}",
    ])

    if problems:
        lines.extend([
            "",
            "ã€æ³¨æ„äº‹é¡¹ã€‘",
        ])
        for p in problems[:2]:
            content = p['content'][:60].replace('\n', ' ')
            lines.append(f"- {content}...")

    lines.extend([
        "",
        f"ã€ç”Ÿæˆæ—¶é—´ã€‘{datetime.now().isoformat()}",
    ])

    return '\n'.join(lines)


def generate_anchor_candidates(
    decisions: list[dict],
    constraints: list[dict],
    problems: list[dict]
) -> list[dict]:
    """ç”Ÿæˆå€™é€‰é”šç‚¹ï¼ˆå¸¦å†…å®¹éªŒè¯ï¼‰"""
    candidates = []

    # å†³ç­–é”šç‚¹
    for i, d in enumerate(decisions):
        content = d['content']
        if not _is_valid_anchor_content(content, 'decision'):
            continue
        candidates.append({
            'id': f'D_candidate_{i}',
            'type': 'decision',
            'title': _extract_title(content, 'decision'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'decision'),
            }
        })

    # é—®é¢˜é”šç‚¹
    for i, p in enumerate(problems):
        content = p['content']
        if not _is_valid_anchor_content(content, 'problem'):
            continue
        candidates.append({
            'id': f'P_candidate_{i}',
            'type': 'problem',
            'title': _extract_title(content, 'problem'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'problem'),
            }
        })

    # çº¦æŸé”šç‚¹
    for i, c in enumerate(constraints):
        content = c['content']
        if not _is_valid_anchor_content(content, 'constraint'):
            continue
        candidates.append({
            'id': f'C_candidate_{i}',
            'type': 'constraint',
            'title': _extract_title(content, 'constraint'),
            'content': content[:300],
            'threshold_check': {
                'frequency': 1,
                'impact': _detect_impact(content),
                'reusable': _detect_reusable(content, 'constraint'),
            }
        })

    return candidates


def _extract_title(content: str, anchor_type: str) -> str:
    """ä»å†…å®¹ä¸­æå–æ ‡é¢˜"""
    lines = content.split('\n')

    # 1. ä¼˜å…ˆæŸ¥æ‰¾ markdown æ ‡é¢˜æ ¼å¼
    for line in lines[:5]:
        line = line.strip()
        md_title = re.match(r'^#{1,3}\s+(.+)$', line)
        if md_title:
            title = md_title.group(1).strip()
            return title[:50] if len(title) <= 50 else title[:47] + '...'

        bold_title = re.match(r'^\*\*(.+?)\*\*', line)
        if bold_title:
            title = bold_title.group(1).strip()
            return title[:50] if len(title) <= 50 else title[:47] + '...'

    # 2. æ’é™¤å¯¹è¯å¼€å¤´ï¼Œæ‰¾ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„è¡Œ
    skip_prefixes = (
        'å®Œæˆ', 'å¥½çš„', 'å¥½ï¼Œ', 'æ˜¯çš„', 'æ²¡é—®é¢˜', 'å·²', 'æˆ‘',
        'Done', 'OK', 'Yes', 'I\'ll', 'I will', 'Let me', 'You are',
        'This', 'The ', 'Here', '```',
    )

    for line in lines[:10]:
        line = line.strip()
        if not line or len(line) < 5:
            continue
        if any(line.startswith(p) for p in skip_prefixes):
            continue
        line = re.sub(r'^[\-\*\d\.]+\s*', '', line)
        return line[:50] if len(line) <= 50 else line[:47] + '...'

    return f'{anchor_type}_untitled'


# ============================================================
# é”šç‚¹å†…å®¹éªŒè¯ (Anchor Content Validation)
# ============================================================

CONVERSATION_PREFIXES = (
    'å®Œæˆ', 'å¥½çš„', 'å¥½ï¼Œ', 'æ˜¯çš„', 'æ²¡é—®é¢˜', 'å·²ç»', 'æˆ‘æ¥', 'æˆ‘ä¼š', 'è®©æˆ‘',
    'ç°åœ¨', 'æ¥ä¸‹æ¥', 'é¦–å…ˆ', 'ç„¶å', 'æœ€å', 'æ€»ç»“',
    'Done', 'OK', 'Yes', 'I\'ll', 'I will', 'I\'m', 'Let me', 'Now',
    'You are', 'This command', 'Here is', 'Here are', '```',
)

ANCHOR_STRUCTURE_KEYWORDS = {
    'decision': ['context', 'decision', 'impact', 'evidence', 'èƒŒæ™¯', 'å†³ç­–', 'å½±å“', 'åŸå› '],
    'problem': ['ç—‡çŠ¶', 'æ ¹å› ', 'è§£å†³', 'é¢„é˜²', 'symptom', 'root cause', 'solution', 'prevention'],
    'constraint': ['çº¦æŸ', 'å¿…é¡»', 'ç¦æ­¢', 'must', 'never', 'always', 'constraint'],
}

IMPACT_KEYWORDS = [
    'æ¶æ„', 'å®‰å…¨', 'æ€§èƒ½', 'å¤šæ¨¡å—', 'å…¨å±€', 'æ ¸å¿ƒ', 'å…³é”®',
    'architecture', 'security', 'performance', 'global', 'core', 'critical',
]


def _is_valid_anchor_content(content: str, anchor_type: str) -> bool:
    """éªŒè¯å†…å®¹æ˜¯å¦é€‚åˆä½œä¸ºé”šç‚¹"""
    if not content or len(content.strip()) < 50:
        return False

    first_line = content.strip().split('\n')[0].strip()
    if any(first_line.startswith(p) for p in CONVERSATION_PREFIXES):
        return False

    keywords = ANCHOR_STRUCTURE_KEYWORDS.get(anchor_type, [])
    if keywords:
        content_lower = content.lower()
        if not any(kw.lower() in content_lower for kw in keywords):
            return False

    return True


def _detect_impact(content: str) -> bool:
    """æ£€æµ‹å†…å®¹æ˜¯å¦æ¶‰åŠé‡å¤§å½±å“"""
    content_lower = content.lower()
    return any(kw.lower() in content_lower for kw in IMPACT_KEYWORDS)


def _detect_reusable(content: str, anchor_type: str) -> bool:
    """æ£€æµ‹å†…å®¹æ˜¯å¦å¯å¤ç”¨"""
    if anchor_type == 'problem':
        return True
    reusable_keywords = ['æ¨¡å¼', 'é€šç”¨', 'æœ€ä½³å®è·µ', 'pattern', 'generic', 'best practice']
    return any(kw.lower() in content.lower() for kw in reusable_keywords)


# ============================================================
# æ…§â†’è¯† äº¤æ¥åè®® (Hui -> Shi Handoff Protocol)
# ============================================================

def generate_hui_output(
    session_id: str,
    project_path: str,
    task: str,
    decisions: list[dict],
    constraints: list[dict],
    interfaces: list[dict],
    problems: list[dict],
    progress: dict,
    compact_context: str,
    candidates: list[dict]
) -> dict:
    """
    ç”Ÿæˆæ…§æ¨¡å—çš„æ ‡å‡†åŒ–è¾“å‡ºï¼ˆJSON æ ¼å¼ï¼‰ã€‚

    è¿™æ˜¯æ…§â†’è¯†çš„äº¤æ¥åè®®ï¼Œå®šä¹‰äº†ä¸¤ä¸ªæ¨¡å—ä¹‹é—´çš„æ•°æ®å¥‘çº¦ã€‚

    Args:
        session_id: ä¼šè¯ID
        project_path: é¡¹ç›®è·¯å¾„
        task: å½“å‰ä»»åŠ¡æè¿°
        decisions: å†³ç­–åˆ—è¡¨
        constraints: çº¦æŸåˆ—è¡¨
        interfaces: æ¥å£åˆ—è¡¨
        problems: é—®é¢˜åˆ—è¡¨
        progress: è¿›åº¦ä¿¡æ¯
        compact_context: ç¼©å½¢æ€ä¸Šä¸‹æ–‡ (markdown)
        candidates: å€™é€‰é”šç‚¹åˆ—è¡¨

    Returns:
        æ ‡å‡†åŒ–çš„æ…§æ¨¡å—è¾“å‡º (dict)
    """
    return {
        "version": "1.0",
        "timestamp": datetime.now().isoformat(),
        "session_id": session_id,
        "project_path": project_path,

        "context": {
            "task": task,
            "decisions": decisions,
            "constraints": constraints,
            "interfaces": interfaces,
            "problems": problems,
            "progress": progress,
            "compact_md": compact_context,
        },

        "anchors": candidates,  # å€™é€‰é”šç‚¹åˆ—è¡¨
    }


# ============================================================
# è¯†æ¨¡å— - å†™å…¥é€»è¾‘ (Shi Module - Write Logic)
# ============================================================

def check_threshold(anchor: dict) -> bool:
    """
    æ£€æŸ¥é”šç‚¹æ˜¯å¦é€šè¿‡å†™å…¥é—¨æ§›ï¼ˆä¸¥æ ¼ç‰ˆï¼‰ã€‚

    é—¨æ§›æ¡ä»¶ (è‡³å°‘æ»¡è¶³ä¸€é¡¹):
    - frequency >= 2: ç±»ä¼¼é—®é¢˜/å†³ç­–å‡ºç°ä¸¤æ¬¡ä»¥ä¸Š
    - impact = True AND å†…å®¹é•¿åº¦ >= 100
    - reusable = True AND anchor_type == 'problem' AND æœ‰è§£å†³æ–¹æ¡ˆ

    Args:
        anchor: é”šç‚¹å­—å…¸ï¼ŒåŒ…å« threshold_check å­—æ®µ

    Returns:
        bool: æ˜¯å¦é€šè¿‡é—¨æ§›
    """
    threshold = anchor.get('threshold_check', {})
    anchor_type = anchor.get('type', '')
    content = anchor.get('content', '')

    # 1. é¢‘ç‡æ£€æŸ¥ (æœ€å¯é )
    frequency = threshold.get('frequency', 0)
    if isinstance(frequency, int) and frequency >= 2:
        return True

    # 2. å½±å“æ£€æŸ¥ (éœ€é…åˆå†…å®¹é•¿åº¦)
    if threshold.get('impact', False) and len(content) >= 100:
        return True

    # 3. å¯å¤ç”¨æ£€æŸ¥ (åªå¯¹ problem ç±»å‹ + æœ‰è§£å†³æ–¹æ¡ˆç”Ÿæ•ˆ)
    if threshold.get('reusable', False) and anchor_type == 'problem':
        content_lower = content.lower()
        has_solution = any(kw in content_lower for kw in ['è§£å†³', 'ä¿®å¤', 'é¢„é˜²', 'fix', 'solution', 'resolve'])
        if has_solution:
            return True

    return False


def check_duplicate(anchor: dict, existing_anchors: list[dict]) -> tuple[bool, str | None]:
    """
    æ£€æŸ¥é”šç‚¹æ˜¯å¦ä¸ç°æœ‰é”šç‚¹é‡å¤ã€‚

    ä½¿ç”¨ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥:
    - æ ‡é¢˜å®Œå…¨ç›¸åŒ -> é‡å¤
    - æ ‡é¢˜è¯æ±‡é‡å  > 70% -> é‡å¤

    Args:
        anchor: å¾…æ£€æŸ¥çš„é”šç‚¹
        existing_anchors: ç°æœ‰é”šç‚¹åˆ—è¡¨

    Returns:
        (is_duplicate, existing_id): æ˜¯å¦é‡å¤åŠé‡å¤é”šç‚¹çš„ID
    """
    new_title = anchor.get('title', '').lower()
    new_words = set(re.findall(r'\w+', new_title))

    if not new_words:
        return False, None

    for existing in existing_anchors:
        existing_title = existing.get('title', '').lower()
        existing_words = set(re.findall(r'\w+', existing_title))

        if not existing_words:
            continue

        # å®Œå…¨ç›¸åŒ
        if new_title == existing_title:
            return True, existing.get('id')

        # è¯æ±‡é‡å æ£€æŸ¥
        intersection = new_words & existing_words
        union = new_words | existing_words

        if union and len(intersection) / len(union) > 0.7:
            return True, existing.get('id')

    return False, None


def _get_next_anchor_id(anchor_type: str, existing_anchors: list[dict]) -> str:
    """
    è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„é”šç‚¹IDã€‚

    IDæ ¼å¼: {type_prefix}{3ä½æ•°å­—}
    - å†³ç­–: D001, D002, ...
    - é—®é¢˜: P001, P002, ...
    - çº¦æŸ: C001, C002, ...
    - æ¥å£: I001, I002, ...

    Args:
        anchor_type: é”šç‚¹ç±»å‹ (decision, problem, constraint, interface)
        existing_anchors: ç°æœ‰é”šç‚¹åˆ—è¡¨

    Returns:
        æ–°çš„é”šç‚¹ID
    """
    type_prefix_map = {
        'decision': 'D',
        'problem': 'P',
        'constraint': 'C',
        'interface': 'I',
    }
    prefix = type_prefix_map.get(anchor_type, 'A')

    # æ‰¾å‡ºåŒç±»å‹çš„æœ€å¤§ID
    max_num = 0
    pattern = re.compile(rf'^{prefix}(\d+)$')

    for anchor in existing_anchors:
        anchor_id = anchor.get('id', '')
        match = pattern.match(anchor_id)
        if match:
            num = int(match.group(1))
            max_num = max(max_num, num)

    return f'{prefix}{max_num + 1:03d}'


def _load_existing_anchors(anchors_path: Path) -> list[dict]:
    """
    ä» anchors.md æ–‡ä»¶åŠ è½½ç°æœ‰é”šç‚¹ã€‚

    è§£ææ ¼å¼:
    ## [D001] å†³ç­–æ ‡é¢˜
    å†…å®¹...

    Args:
        anchors_path: anchors.md æ–‡ä»¶è·¯å¾„

    Returns:
        é”šç‚¹åˆ—è¡¨
    """
    if not anchors_path.exists():
        return []

    anchors = []
    content = anchors_path.read_text(encoding='utf-8')

    # åŒ¹é… ## [ID] æ ‡é¢˜ æ ¼å¼
    anchor_pattern = re.compile(
        r'^## \[([A-Z]\d+)\] (.+?)$\n(.*?)(?=^## \[|$)',
        re.MULTILINE | re.DOTALL
    )

    for match in anchor_pattern.finditer(content):
        anchor_id = match.group(1)
        title = match.group(2).strip()
        body = match.group(3).strip()

        # æ¨æ–­ç±»å‹
        anchor_type = 'unknown'
        if anchor_id.startswith('D'):
            anchor_type = 'decision'
        elif anchor_id.startswith('P'):
            anchor_type = 'problem'
        elif anchor_id.startswith('C'):
            anchor_type = 'constraint'
        elif anchor_id.startswith('I'):
            anchor_type = 'interface'

        anchors.append({
            'id': anchor_id,
            'type': anchor_type,
            'title': title,
            'content': body,
        })

    return anchors


def write_to_anchors(anchor: dict, anchors_path: Path) -> str:
    """
    å°†é”šç‚¹è¿½åŠ åˆ° anchors.md æ–‡ä»¶ã€‚

    Args:
        anchor: é”šç‚¹å­—å…¸
        anchors_path: anchors.md æ–‡ä»¶è·¯å¾„

    Returns:
        æ–°åˆ†é…çš„é”šç‚¹ID
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    anchors_path.parent.mkdir(parents=True, exist_ok=True)

    # åŠ è½½ç°æœ‰é”šç‚¹
    existing_anchors = _load_existing_anchors(anchors_path)

    # åˆ†é…æ–°ID
    new_id = _get_next_anchor_id(anchor.get('type', 'unknown'), existing_anchors)

    # æ„å»ºé”šç‚¹å†…å®¹
    title = anchor.get('title', 'Untitled')
    content = anchor.get('content', '')
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')

    anchor_entry = f"""
## [{new_id}] {title}

**åˆ›å»ºæ—¶é—´**: {timestamp}
**ç±»å‹**: {anchor.get('type', 'unknown')}

{content}

---
"""

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºå¤´éƒ¨
    if not anchors_path.exists():
        header = """# é”šç‚¹è®°å½• (Anchors)

> æ­¤æ–‡ä»¶ç”±è¯†æ¨¡å—è‡ªåŠ¨ç»´æŠ¤ï¼Œè®°å½•è·¨ä¼šè¯çš„é‡è¦å†³ç­–ã€é—®é¢˜å’Œçº¦æŸã€‚

"""
        with open(anchors_path, 'w', encoding='utf-8') as f:
            f.write(header)

    # è¿½åŠ é”šç‚¹
    with open(anchors_path, 'a', encoding='utf-8') as f:
        f.write(anchor_entry)

    return new_id


def _get_project_hash(project_path: str) -> str:
    """è®¡ç®—é¡¹ç›®è·¯å¾„çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç´¢å¼•ï¼‰"""
    return hashlib.md5(project_path.encode()).hexdigest()[:8]


def _get_project_name(project_path: str) -> str:
    """ä»é¡¹ç›®è·¯å¾„æå–é¡¹ç›®å"""
    return Path(project_path).name or 'unknown'


def update_session_index(session_info: dict, index_path: Path):
    """
    æ›´æ–°ä¼šè¯ç´¢å¼•æ–‡ä»¶ (index.json)ã€‚

    ç´¢å¼•ç»“æ„:
    {
        "version": "1.0",
        "sessions": [...],
        "projects": {...}
    }

    Args:
        session_info: ä¼šè¯ä¿¡æ¯å­—å…¸
        index_path: index.json æ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    index_path.parent.mkdir(parents=True, exist_ok=True)

    # åŠ è½½ç°æœ‰ç´¢å¼•
    index = {
        "version": "1.0",
        "sessions": [],
        "projects": {}
    }

    if index_path.exists():
        try:
            with open(index_path, 'r', encoding='utf-8') as f:
                index = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    # ç¡®ä¿ç»“æ„å®Œæ•´
    if "sessions" not in index:
        index["sessions"] = []
    if "projects" not in index:
        index["projects"] = {}

    session_id = session_info.get('session_id', 'unknown')
    project_path = session_info.get('project_path', '.')
    project_hash = _get_project_hash(project_path)
    project_name = _get_project_name(project_path)
    now = datetime.now().isoformat()

    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ä¼šè¯
    existing_session = None
    for i, s in enumerate(index["sessions"]):
        if s.get('session_id') == session_id:
            existing_session = i
            break

    session_entry = {
        "session_id": session_id,
        "project_path": project_path,
        "project_hash": project_hash,
        "created_at": session_info.get('created_at', now),
        "updated_at": now,
        "task_summary": session_info.get('task_summary', '')[:200],
        "anchor_count": session_info.get('anchor_count', 0),
        "status": session_info.get('status', 'active')
    }

    if existing_session is not None:
        # æ›´æ–°ç°æœ‰ä¼šè¯
        session_entry["created_at"] = index["sessions"][existing_session].get('created_at', now)
        index["sessions"][existing_session] = session_entry
    else:
        # æ·»åŠ æ–°ä¼šè¯
        index["sessions"].append(session_entry)

    # æ›´æ–°é¡¹ç›®ä¿¡æ¯
    if project_hash not in index["projects"]:
        index["projects"][project_hash] = {
            "path": project_path,
            "name": project_name,
            "session_count": 0
        }

    # ç»Ÿè®¡è¯¥é¡¹ç›®çš„ä¼šè¯æ•°
    project_sessions = sum(
        1 for s in index["sessions"]
        if s.get('project_hash') == project_hash
    )
    index["projects"][project_hash]["session_count"] = project_sessions

    # ä¿å­˜ç´¢å¼•
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def shi_write(hui_output: dict, cwd: str) -> dict:
    """
    è¯†æ¨¡å—çš„ä¸»å†™å…¥å‡½æ•°ã€‚

    æ¥æ”¶æ…§æ¨¡å—çš„ JSON è¾“å‡ºï¼Œæ‰§è¡Œ:
    1. é—¨æ§›æ£€æŸ¥
    2. å»é‡æ£€æŸ¥
    3. å†™å…¥é€šè¿‡çš„é”šç‚¹
    4. æ›´æ–°ç´¢å¼•

    Args:
        hui_output: æ…§æ¨¡å—çš„æ ‡å‡†åŒ–è¾“å‡º
        cwd: å½“å‰å·¥ä½œç›®å½•

    Returns:
        å†™å…¥ç»“æœæ‘˜è¦
    """
    result = {
        "anchors_written": [],
        "anchors_skipped": [],
        "anchors_duplicated": [],
        "errors": []
    }

    # å‡†å¤‡è·¯å¾„
    wukong_dir = Path(cwd) / '.wukong'
    anchors_path = wukong_dir / 'context' / 'anchors.md'
    index_path = wukong_dir / 'context' / 'index.json'

    # åŠ è½½ç°æœ‰é”šç‚¹
    existing_anchors = _load_existing_anchors(anchors_path)

    # å¤„ç†å€™é€‰é”šç‚¹
    candidates = hui_output.get('anchors', [])

    for candidate in candidates:
        try:
            # 1. é—¨æ§›æ£€æŸ¥
            if not check_threshold(candidate):
                result["anchors_skipped"].append({
                    "id": candidate.get('id'),
                    "reason": "threshold_not_met"
                })
                continue

            # 2. å»é‡æ£€æŸ¥
            is_dup, existing_id = check_duplicate(candidate, existing_anchors)
            if is_dup:
                result["anchors_duplicated"].append({
                    "id": candidate.get('id'),
                    "existing_id": existing_id
                })
                continue

            # 3. å†™å…¥é”šç‚¹
            new_id = write_to_anchors(candidate, anchors_path)
            result["anchors_written"].append({
                "candidate_id": candidate.get('id'),
                "new_id": new_id,
                "type": candidate.get('type'),
                "title": candidate.get('title')
            })

            # æ›´æ–°ç°æœ‰é”šç‚¹åˆ—è¡¨ï¼ˆç”¨äºåç»­å»é‡ï¼‰
            existing_anchors.append({
                'id': new_id,
                'type': candidate.get('type'),
                'title': candidate.get('title'),
                'content': candidate.get('content')
            })

        except Exception as e:
            result["errors"].append({
                "id": candidate.get('id'),
                "error": str(e)
            })

    # 4. æ›´æ–°ä¼šè¯ç´¢å¼•
    try:
        session_info = {
            "session_id": hui_output.get('session_id', 'unknown'),
            "project_path": hui_output.get('project_path', cwd),
            "task_summary": hui_output.get('context', {}).get('task', ''),
            "anchor_count": len(result["anchors_written"]),
            "status": "active"
        }
        update_session_index(session_info, index_path)
    except Exception as e:
        result["errors"].append({
            "id": "session_index",
            "error": str(e)
        })

    return result


def save_context(cwd: str, compact_context: str, session_id: str):
    """ä¿å­˜ä¸Šä¸‹æ–‡åˆ°æ–‡ä»¶"""
    context_dir = Path(cwd) / '.wukong' / 'context' / 'current'
    context_dir.mkdir(parents=True, exist_ok=True)

    compact_path = context_dir / 'compact.md'
    with open(compact_path, 'w', encoding='utf-8') as f:
        f.write(compact_context)

    # åŒæ—¶ä¿å­˜ä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
    sessions_dir = Path(cwd) / '.wukong' / 'context' / 'sessions'
    sessions_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    session_dir = sessions_dir / f'{timestamp}-{session_id[:8]}'
    session_dir.mkdir(parents=True, exist_ok=True)

    backup_path = session_dir / 'compact.md'
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(compact_context)


def output_to_claude(compact_context: str, candidates: list[dict]):
    """è¾“å‡ºç»™ Claude (ä¼šè¢«æ³¨å…¥åˆ°å‹ç¼©åçš„ä¸Šä¸‹æ–‡)"""
    print("## [æ…§] PreCompact æå–å®Œæˆ")
    print()
    print("å·²ä¿å­˜å…³é”®ä¸Šä¸‹æ–‡åˆ° `.wukong/context/current/compact.md`")
    print()
    if candidates:
        print(f"è¯†åˆ«åˆ° {len(candidates)} ä¸ªå€™é€‰é”šç‚¹ï¼Œå¾…åç»­é—¨æ§›æ£€æŸ¥ã€‚")
    print()
    print("å¦‚éœ€æ¢å¤è¯¦ç»†ä¿¡æ¯ï¼Œè¯»å– `.wukong/context/sessions/` ä¸‹å¯¹åº”æ–‡ä»¶ã€‚")


def main():
    """
    ä¸»å…¥å£å‡½æ•°ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼:
    1. PreCompact Hook æ¨¡å¼: ä» stdin è¯»å– hook è¾“å…¥ï¼Œæå–ä¸Šä¸‹æ–‡
    2. åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼: é€šè¿‡ç¯å¢ƒå˜é‡ WUKONG_COMPRESS_AVATAR=1 è§¦å‘

    ç¯å¢ƒå˜é‡:
    - WUKONG_COMPRESS_AVATAR: è®¾ä¸º 1 å¯ç”¨åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼
    - WUKONG_AVATAR_TYPE: åˆ†èº«ç±»å‹ (çœ¼/è€³/é¼»/èˆŒ/èº«/æ„ æˆ–è‹±æ–‡åˆ«å)
    - WUKONG_AVATAR_OUTPUT: è¦å‹ç¼©çš„è¾“å‡ºå†…å®¹ (æˆ–ä» stdin è¯»å–)
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼
    if os.environ.get('WUKONG_COMPRESS_AVATAR') == '1':
        _run_avatar_compress_mode()
        return

    # åŸæœ‰çš„ PreCompact Hook æ¨¡å¼
    _run_precompact_mode()


def _run_avatar_compress_mode():
    """åˆ†èº«è¾“å‡ºå‹ç¼©æ¨¡å¼"""
    avatar_type = os.environ.get('WUKONG_AVATAR_TYPE', 'default')

    # ä»ç¯å¢ƒå˜é‡æˆ– stdin è¯»å–è¾“å‡ºå†…å®¹
    avatar_output = os.environ.get('WUKONG_AVATAR_OUTPUT', '')
    if not avatar_output:
        avatar_output = sys.stdin.read()

    if not avatar_output:
        print("## [æ…§] æ— åˆ†èº«è¾“å‡ºå¯å‹ç¼©", file=sys.stderr)
        return

    # å‹ç¼©è¾“å‡º
    compressed = compress_avatar_output(avatar_output, avatar_type)

    # è¾“å‡ºå‹ç¼©ç»“æœ
    print(compressed)

    # è®°å½•æ—¥å¿—
    log_path = Path.home() / '.wukong' / 'hooks' / 'hui-compress.log'
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"\n--- {datetime.now().isoformat()} ---\n")
        log.write(f"Avatar: {avatar_type}\n")
        log.write(f"Original: {len(avatar_output)} chars\n")
        log.write(f"Compressed: {len(compressed)} chars\n")
        log.write(f"Ratio: {len(compressed)/len(avatar_output)*100:.1f}%\n")


def _run_precompact_mode():
    """PreCompact Hook æ¨¡å¼"""
    # 1. è¯»å– hook è¾“å…¥
    hook_input = read_hook_input()

    # Debug: è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_path = Path.home() / '.wukong' / 'hooks' / 'hui-extract.log'
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"\n--- {datetime.now().isoformat()} ---\n")
        log.write(f"Input keys: {list(hook_input.keys())}\n")
        log.write(f"Full input: {json.dumps(hook_input, ensure_ascii=False)[:500]}\n")

    print(f"## [æ…§] Hook è§¦å‘ - è¾“å…¥: {list(hook_input.keys())}", file=sys.stderr)

    transcript_path = hook_input.get('transcript_path', '')
    session_id = hook_input.get('session_id', 'unknown')
    cwd = hook_input.get('cwd', '.')
    trigger = hook_input.get('trigger', 'unknown')

    print(f"## [æ…§] trigger={trigger}, session={session_id[:8] if session_id else 'none'}", file=sys.stderr)

    # 2. è¯»å–å¯¹è¯è®°å½•
    messages = read_transcript(transcript_path)

    if not messages:
        print("## [æ…§] æ— å¯¹è¯è®°å½•å¯æå–")
        return

    # 3. æå–å…³é”®ä¿¡æ¯
    task = extract_current_task(messages)
    decisions = extract_decisions(messages)
    constraints = extract_constraints(messages)
    interfaces = extract_interfaces(messages)
    problems = extract_problems(messages)
    progress = extract_progress(messages)

    # 4. ç”Ÿæˆç¼©å½¢æ€ä¸Šä¸‹æ–‡
    compact_context = generate_compact_context(
        task=task,
        decisions=decisions,
        constraints=constraints,
        interfaces=interfaces,
        problems=problems,
        progress=progress
    )

    # 5. ç”Ÿæˆå€™é€‰é”šç‚¹
    candidates = generate_anchor_candidates(decisions, constraints, problems)

    # 6. ä¿å­˜åˆ°æ–‡ä»¶
    save_context(cwd, compact_context, session_id)

    # 7. ç”Ÿæˆæ…§æ¨¡å—æ ‡å‡†è¾“å‡º (æ…§â†’è¯†äº¤æ¥åè®®)
    hui_output = generate_hui_output(
        session_id=session_id,
        project_path=cwd,
        task=task,
        decisions=decisions,
        constraints=constraints,
        interfaces=interfaces,
        problems=problems,
        progress=progress,
        compact_context=compact_context,
        candidates=candidates
    )

    # 8. ä¿å­˜æ…§è¾“å‡º JSON (ä¾›è°ƒè¯•å’Œè¯†æ¨¡å—ä½¿ç”¨)
    sessions_dir = Path(cwd) / '.wukong' / 'context' / 'sessions'
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    session_dir = sessions_dir / f'{timestamp}-{session_id[:8]}'
    session_dir.mkdir(parents=True, exist_ok=True)

    hui_output_path = session_dir / 'hui-output.json'
    with open(hui_output_path, 'w', encoding='utf-8') as f:
        json.dump(hui_output, f, ensure_ascii=False, indent=2)

    # 9. è°ƒç”¨è¯†æ¨¡å—å†™å…¥
    shi_result = shi_write(hui_output, cwd)

    # è®°å½•è¯†æ¨¡å—ç»“æœ
    shi_result_path = session_dir / 'shi-result.json'
    with open(shi_result_path, 'w', encoding='utf-8') as f:
        json.dump(shi_result, f, ensure_ascii=False, indent=2)

    # è®°å½•æ—¥å¿—
    with open(log_path, 'a', encoding='utf-8') as log:
        log.write(f"Shi result: written={len(shi_result['anchors_written'])}, ")
        log.write(f"skipped={len(shi_result['anchors_skipped'])}, ")
        log.write(f"duplicated={len(shi_result['anchors_duplicated'])}\n")

    # 10. è¾“å‡ºç»™ Claude
    output_to_claude_with_shi_result(compact_context, candidates, shi_result)


def output_to_claude_with_shi_result(
    compact_context: str,
    candidates: list[dict],
    shi_result: dict
):
    """è¾“å‡ºç»™ Claudeï¼ŒåŒ…å«è¯†æ¨¡å—çš„å†™å…¥ç»“æœ"""
    print("## [æ…§] PreCompact æå–å®Œæˆ")
    print()
    print("å·²ä¿å­˜å…³é”®ä¸Šä¸‹æ–‡åˆ° `.wukong/context/current/compact.md`")
    print()

    if candidates:
        print(f"è¯†åˆ«åˆ° {len(candidates)} ä¸ªå€™é€‰é”šç‚¹ã€‚")

    # æ˜¾ç¤ºè¯†æ¨¡å—å†™å…¥ç»“æœ
    written = shi_result.get('anchors_written', [])
    skipped = shi_result.get('anchors_skipped', [])
    duplicated = shi_result.get('anchors_duplicated', [])

    if written:
        print(f"\n### [è¯†] å·²å†™å…¥ {len(written)} ä¸ªé”šç‚¹:")
        for w in written[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            print(f"  - [{w['new_id']}] {w.get('title', '')[:40]}")

    if skipped:
        print(f"\n### [è¯†] è·³è¿‡ {len(skipped)} ä¸ªé”šç‚¹ (æœªè¾¾é—¨æ§›)")

    if duplicated:
        print(f"\n### [è¯†] å»é‡ {len(duplicated)} ä¸ªé”šç‚¹ (å·²å­˜åœ¨)")

    errors = shi_result.get('errors', [])
    if errors:
        print(f"\n### [è¯†] é”™è¯¯: {len(errors)} ä¸ª")
        for e in errors[:3]:
            print(f"  - {e.get('id')}: {e.get('error')}")

    print()
    print("å¦‚éœ€æ¢å¤è¯¦ç»†ä¿¡æ¯ï¼Œè¯»å– `.wukong/context/sessions/` ä¸‹å¯¹åº”æ–‡ä»¶ã€‚")


if __name__ == '__main__':
    main()
